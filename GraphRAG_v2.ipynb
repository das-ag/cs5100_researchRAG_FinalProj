{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CITATION: \n",
    "## [GraphRAG Implementation with LlamaIndex - V2](https://docs.llamaindex.ai/en/stable/examples/cookbooks/GraphRAG_v2/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>801.0341</td>\n",
       "      <td>Michael Chertkov</td>\n",
       "      <td>Michael Chertkov (Los Alamos)</td>\n",
       "      <td>Exactness of Belief Propagation for Some Graph...</td>\n",
       "      <td>12 pages, 1 figure, submitted to JSTAT</td>\n",
       "      <td>J. Stat. Mech. (2008) P10016</td>\n",
       "      <td>10.1088/1742-5468/2008/10/P10016</td>\n",
       "      <td>LANL LA-UR-07-8441</td>\n",
       "      <td>cond-mat.stat-mech cond-mat.other cs.AI cs.IT ...</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>It is well known that an arbitrary graphical m...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Wed, 2 Jan 2008...</td>\n",
       "      <td>2009-11-13</td>\n",
       "      <td>[[Chertkov, Michael, , Los Alamos]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>803.4355</td>\n",
       "      <td>Marko A. Rodriguez</td>\n",
       "      <td>Marko A. Rodriguez</td>\n",
       "      <td>Grammar-Based Random Walkers in Semantic Networks</td>\n",
       "      <td>First draft of manuscript originally written i...</td>\n",
       "      <td>Rodriguez, M.A., \"Grammar-Based Random Walkers...</td>\n",
       "      <td>10.1016/j.knosys.2008.03.030</td>\n",
       "      <td>LA-UR-06-7791</td>\n",
       "      <td>cs.AI cs.DS</td>\n",
       "      <td>http://creativecommons.org/licenses/publicdomain/</td>\n",
       "      <td>Semantic networks qualify the meaning of an ed...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 31 Mar 200...</td>\n",
       "      <td>2008-09-11</td>\n",
       "      <td>[[Rodriguez, Marko A., ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>810.2434</td>\n",
       "      <td>Edward Rosten</td>\n",
       "      <td>Edward Rosten, Reid Porter, Tom Drummond</td>\n",
       "      <td>Faster and better: a machine learning approach...</td>\n",
       "      <td>35 pages, 11 figures</td>\n",
       "      <td>IEEE Trans. PAMI, 32 (2010), 105--119</td>\n",
       "      <td>10.1109/TPAMI.2008.275</td>\n",
       "      <td>07-3912</td>\n",
       "      <td>cs.CV cs.LG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>The repeatability and efficiency of a corner d...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Tue, 14 Oct 200...</td>\n",
       "      <td>2010-07-09</td>\n",
       "      <td>[[Rosten, Edward, ], [Porter, Reid, ], [Drummo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>812.4446</td>\n",
       "      <td>Peter Turney</td>\n",
       "      <td>Peter D. Turney (National Research Council of ...</td>\n",
       "      <td>The Latent Relation Mapping Engine: Algorithm ...</td>\n",
       "      <td>related work available at http://purl.org/pete...</td>\n",
       "      <td>Journal of Artificial Intelligence Research, (...</td>\n",
       "      <td>10.1613/jair.2693</td>\n",
       "      <td>NRC-50738</td>\n",
       "      <td>cs.CL cs.AI cs.LG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>Many AI researchers and cognitive scientists h...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Tue, 23 Dec 200...</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>[[Turney, Peter D., , National Research Counci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>901.3574</td>\n",
       "      <td>Christoph Benzmueller</td>\n",
       "      <td>Christoph Benzmueller</td>\n",
       "      <td>Automating Access Control Logics in Simple Typ...</td>\n",
       "      <td>ii + 20 pages</td>\n",
       "      <td>SEKI Report SR-2008-01 (ISSN 1437-4447), Saarl...</td>\n",
       "      <td>10.1007/978-3-642-01244-0_34</td>\n",
       "      <td>SEKI Report SR-2008-01</td>\n",
       "      <td>cs.LO cs.AI</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>Garg and Abadi recently proved that prominent ...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Fri, 23 Jan 200...</td>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>[[Benzmueller, Christoph, ]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id              submitter  \\\n",
       "0  801.0341       Michael Chertkov   \n",
       "1  803.4355     Marko A. Rodriguez   \n",
       "2  810.2434          Edward Rosten   \n",
       "3  812.4446           Peter Turney   \n",
       "4  901.3574  Christoph Benzmueller   \n",
       "\n",
       "                                             authors  \\\n",
       "0                      Michael Chertkov (Los Alamos)   \n",
       "1                                 Marko A. Rodriguez   \n",
       "2           Edward Rosten, Reid Porter, Tom Drummond   \n",
       "3  Peter D. Turney (National Research Council of ...   \n",
       "4                              Christoph Benzmueller   \n",
       "\n",
       "                                               title  \\\n",
       "0  Exactness of Belief Propagation for Some Graph...   \n",
       "1  Grammar-Based Random Walkers in Semantic Networks   \n",
       "2  Faster and better: a machine learning approach...   \n",
       "3  The Latent Relation Mapping Engine: Algorithm ...   \n",
       "4  Automating Access Control Logics in Simple Typ...   \n",
       "\n",
       "                                            comments  \\\n",
       "0             12 pages, 1 figure, submitted to JSTAT   \n",
       "1  First draft of manuscript originally written i...   \n",
       "2                               35 pages, 11 figures   \n",
       "3  related work available at http://purl.org/pete...   \n",
       "4                                      ii + 20 pages   \n",
       "\n",
       "                                         journal-ref  \\\n",
       "0                       J. Stat. Mech. (2008) P10016   \n",
       "1  Rodriguez, M.A., \"Grammar-Based Random Walkers...   \n",
       "2              IEEE Trans. PAMI, 32 (2010), 105--119   \n",
       "3  Journal of Artificial Intelligence Research, (...   \n",
       "4  SEKI Report SR-2008-01 (ISSN 1437-4447), Saarl...   \n",
       "\n",
       "                                doi               report-no  \\\n",
       "0  10.1088/1742-5468/2008/10/P10016      LANL LA-UR-07-8441   \n",
       "1      10.1016/j.knosys.2008.03.030           LA-UR-06-7791   \n",
       "2            10.1109/TPAMI.2008.275                 07-3912   \n",
       "3                 10.1613/jair.2693               NRC-50738   \n",
       "4      10.1007/978-3-642-01244-0_34  SEKI Report SR-2008-01   \n",
       "\n",
       "                                          categories  \\\n",
       "0  cond-mat.stat-mech cond-mat.other cs.AI cs.IT ...   \n",
       "1                                        cs.AI cs.DS   \n",
       "2                                        cs.CV cs.LG   \n",
       "3                                  cs.CL cs.AI cs.LG   \n",
       "4                                        cs.LO cs.AI   \n",
       "\n",
       "                                             license  \\\n",
       "0  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "1  http://creativecommons.org/licenses/publicdomain/   \n",
       "2  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "3  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "4  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  It is well known that an arbitrary graphical m...   \n",
       "1  Semantic networks qualify the meaning of an ed...   \n",
       "2  The repeatability and efficiency of a corner d...   \n",
       "3  Many AI researchers and cognitive scientists h...   \n",
       "4  Garg and Abadi recently proved that prominent ...   \n",
       "\n",
       "                                            versions update_date  \\\n",
       "0  [{'version': 'v1', 'created': 'Wed, 2 Jan 2008...  2009-11-13   \n",
       "1  [{'version': 'v1', 'created': 'Mon, 31 Mar 200...  2008-09-11   \n",
       "2  [{'version': 'v1', 'created': 'Tue, 14 Oct 200...  2010-07-09   \n",
       "3  [{'version': 'v1', 'created': 'Tue, 23 Dec 200...  2020-08-20   \n",
       "4  [{'version': 'v1', 'created': 'Fri, 23 Jan 200...  2015-05-13   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0                [[Chertkov, Michael, , Los Alamos]]  \n",
       "1                          [[Rodriguez, Marko A., ]]  \n",
       "2  [[Rosten, Edward, ], [Porter, Reid, ], [Drummo...  \n",
       "3  [[Turney, Peter D., , National Research Counci...  \n",
       "4                       [[Benzmueller, Christoph, ]]  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llama_index.core import Document\n",
    "\n",
    "json_path =  \"datasets/arxiv_cs_metadata.json\"\n",
    "nrows = 5\n",
    "papers = pd.read_json(json_path, lines=True, nrows=nrows)\n",
    "database = \"arxivcs-demo\"\n",
    "\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(text=f\"{row['title']}: {row['abstract']}\",)\n",
    "    for i, row in papers.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"qwen2.5\",  request_timeout=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from typing import Any, List, Callable, Optional, Union, Dict\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "from llama_index.core.indices.property_graph.utils import (\n",
    "    default_parse_triplets_fn,\n",
    ")\n",
    "from llama_index.core.graph_stores.types import (\n",
    "    EntityNode,\n",
    "    KG_NODES_KEY,\n",
    "    KG_RELATIONS_KEY,\n",
    "    Relation,\n",
    ")\n",
    "from llama_index.core.llms.llm import LLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.schema import TransformComponent, BaseNode\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GraphRAGExtractor(TransformComponent):\n",
    "    \"\"\"Extract triples from a graph.\n",
    "\n",
    "    Uses an LLM and a simple prompt + output parsing to extract paths (i.e. triples) and entity, relation descriptions from text.\n",
    "\n",
    "    Args:\n",
    "        llm (LLM):\n",
    "            The language model to use.\n",
    "        extract_prompt (Union[str, PromptTemplate]):\n",
    "            The prompt to use for extracting triples.\n",
    "        parse_fn (callable):\n",
    "            A function to parse the output of the language model.\n",
    "        num_workers (int):\n",
    "            The number of workers to use for parallel processing.\n",
    "        max_paths_per_chunk (int):\n",
    "            The maximum number of paths to extract per chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    llm: LLM\n",
    "    extract_prompt: PromptTemplate\n",
    "    parse_fn: Callable\n",
    "    num_workers: int\n",
    "    max_paths_per_chunk: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[LLM] = llm,\n",
    "        extract_prompt: Optional[Union[str, PromptTemplate]] = None,\n",
    "        parse_fn: Callable = default_parse_triplets_fn,\n",
    "        max_paths_per_chunk: int = 10,\n",
    "        num_workers: int = 4,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        from llama_index.core import Settings\n",
    "\n",
    "        if isinstance(extract_prompt, str):\n",
    "            extract_prompt = PromptTemplate(extract_prompt)\n",
    "\n",
    "        super().__init__(\n",
    "            llm=llm or Settings.llm,\n",
    "            extract_prompt=extract_prompt,\n",
    "            parse_fn=parse_fn,\n",
    "            num_workers=num_workers,\n",
    "            max_paths_per_chunk=max_paths_per_chunk,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"GraphExtractor\"\n",
    "\n",
    "    def __call__(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes.\"\"\"\n",
    "        return asyncio.run(\n",
    "            self.acall(nodes, show_progress=show_progress, **kwargs)\n",
    "        )\n",
    "\n",
    "    async def _aextract(self, node: BaseNode) -> BaseNode:\n",
    "        \"\"\"Extract triples from a node.\"\"\"\n",
    "        assert hasattr(node, \"text\")\n",
    "\n",
    "        text = node.get_content(metadata_mode=\"llm\")\n",
    "        try:\n",
    "            llm_response = await self.llm.apredict(\n",
    "                self.extract_prompt,\n",
    "                text=text,\n",
    "                max_knowledge_triplets=self.max_paths_per_chunk,\n",
    "            )\n",
    "            print(f\"llm_response: {llm_response}\")\n",
    "            entities, entities_relationship = self.parse_fn(llm_response)\n",
    "        except ValueError:\n",
    "            entities = []\n",
    "            entities_relationship = []\n",
    "\n",
    "        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])\n",
    "        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])\n",
    "        entity_metadata = node.metadata.copy()\n",
    "        for entity, entity_type, description in entities:\n",
    "            entity_metadata[\"entity_description\"] = description\n",
    "            entity_node = EntityNode(\n",
    "                name=entity, label=entity_type, properties=entity_metadata\n",
    "            )\n",
    "            existing_nodes.append(entity_node)\n",
    "\n",
    "        relation_metadata = node.metadata.copy()\n",
    "        for triple in entities_relationship:\n",
    "            subj, obj, rel, description = triple\n",
    "            relation_metadata[\"relationship_description\"] = description\n",
    "            rel_node = Relation(\n",
    "                label=rel,\n",
    "                source_id=subj,\n",
    "                target_id=obj,\n",
    "                properties=relation_metadata,\n",
    "            )\n",
    "\n",
    "            existing_relations.append(rel_node)\n",
    "\n",
    "        node.metadata[KG_NODES_KEY] = existing_nodes\n",
    "        node.metadata[KG_RELATIONS_KEY] = existing_relations\n",
    "        return node\n",
    "\n",
    "    async def acall(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes async.\"\"\"\n",
    "        jobs = []\n",
    "        for node in nodes:\n",
    "            jobs.append(self._aextract(node))\n",
    "\n",
    "        return await run_jobs(\n",
    "            jobs,\n",
    "            workers=self.num_workers,\n",
    "            show_progress=show_progress,\n",
    "            desc=\"Extracting paths from text\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "from graspologic.partition import hierarchical_leiden\n",
    "from collections import defaultdict\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "\n",
    "\n",
    "class GraphRAGStore(Neo4jPropertyGraphStore):\n",
    "    community_summary = {}\n",
    "    entity_info = None\n",
    "    max_cluster_size = 5\n",
    "    llm = llm\n",
    "\n",
    "    def generate_community_summary(self, text):\n",
    "        \"\"\"Generate summary for a given text using an LLM.\"\"\"\n",
    "        messages = [\n",
    "            ChatMessage(\n",
    "                role=\"system\",\n",
    "                content=(\n",
    "                    \"You are provided with a set of relationships from a knowledge graph, each represented as \"\n",
    "                    \"(relationship$$$$<source_entity>$$$$<target_entity>$$$$<relation>$$$$<relationship_description>).\" \n",
    "                    \"Your task is to create a summary of these relationships. The summary should include the names of the entities involved and a concise synthesis \"\n",
    "                    \"of the relationship descriptions. The goal is to capture the most critical and relevant details that \"\n",
    "                    \"highlight the nature and significance of each relationship. Ensure that the summary is coherent and \"\n",
    "                    \"integrates the information in a way that emphasizes the key aspects of the relationships.\"\n",
    "                ),\n",
    "            ),\n",
    "            ChatMessage(role=\"user\", content=text),\n",
    "        ]\n",
    "        response = llm.chat(messages)\n",
    "        clean_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return clean_response\n",
    "\n",
    "    def build_communities(self):\n",
    "        \"\"\"Builds communities from the graph and summarizes them.\"\"\"\n",
    "        nx_graph = self._create_nx_graph()\n",
    "        community_hierarchical_clusters = hierarchical_leiden(\n",
    "            nx_graph, max_cluster_size=self.max_cluster_size\n",
    "        )\n",
    "        self.entity_info, community_info = self._collect_community_info(\n",
    "            nx_graph, community_hierarchical_clusters\n",
    "        )\n",
    "        self._summarize_communities(community_info)\n",
    "\n",
    "    def _create_nx_graph(self):\n",
    "        \"\"\"Converts internal graph representation to NetworkX graph.\"\"\"\n",
    "        nx_graph = nx.Graph()\n",
    "        triplets = self.get_triplets()\n",
    "        for entity1, relation, entity2 in triplets:\n",
    "            # if relation.properties.get(\"relationship_description\"):\n",
    "\n",
    "                # relation.properties[\"relationship_description\"] = \"\"\n",
    "            relationship_desc = relation.properties.get(\"relationship_description\", \"relationship_description_dummy\")\n",
    "            nx_graph.add_node(entity1.name)\n",
    "            nx_graph.add_node(entity2.name)\n",
    "            nx_graph.add_edge(\n",
    "                relation.source_id,\n",
    "                relation.target_id,\n",
    "                relationship=relation.label,\n",
    "                description=relationship_desc,\n",
    "            )\n",
    "        return nx_graph\n",
    "\n",
    "    def _collect_community_info(self, nx_graph, clusters):\n",
    "        \"\"\"\n",
    "        Collect information for each node based on their community,\n",
    "        allowing entities to belong to multiple clusters.\n",
    "        \"\"\"\n",
    "        entity_info = defaultdict(set)\n",
    "        community_info = defaultdict(list)\n",
    "\n",
    "        for item in clusters:\n",
    "            node = item.node\n",
    "            cluster_id = item.cluster\n",
    "\n",
    "            # Update entity_info\n",
    "            entity_info[node].add(cluster_id)\n",
    "\n",
    "            for neighbor in nx_graph.neighbors(node):\n",
    "                edge_data = nx_graph.get_edge_data(node, neighbor)\n",
    "                if edge_data:\n",
    "                    detail = f\"{node} -> {neighbor} -> {edge_data['relationship']} -> {edge_data['description']}\"\n",
    "                    community_info[cluster_id].append(detail)\n",
    "\n",
    "        # Convert sets to lists for easier serialization if needed\n",
    "        entity_info = {k: list(v) for k, v in entity_info.items()}\n",
    "\n",
    "        return dict(entity_info), dict(community_info)\n",
    "\n",
    "    def _summarize_communities(self, community_info):\n",
    "        \"\"\"Generate and store summaries for each community.\"\"\"\n",
    "        for community_id, details in community_info.items():\n",
    "            details_text = (\n",
    "                \"\\n\".join(details) + \".\"\n",
    "            )  # Ensure it ends with a period\n",
    "            self.community_summary[\n",
    "                community_id\n",
    "            ] = self.generate_community_summary(details_text)\n",
    "\n",
    "    def get_community_summaries(self):\n",
    "        \"\"\"Returns the community summaries, building them if not already done.\"\"\"\n",
    "        if not self.community_summary:\n",
    "            self.build_communities()\n",
    "        return self.community_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.llms import LLM\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "class GraphRAGQueryEngine(CustomQueryEngine):\n",
    "    graph_store: GraphRAGStore\n",
    "    index: PropertyGraphIndex\n",
    "    llm: LLM = llm\n",
    "    similarity_top_k: int = 20\n",
    "\n",
    "    def custom_query(self, query_str: str) -> str:\n",
    "        \"\"\"Process all community summaries to generate answers to a specific query.\"\"\"\n",
    "\n",
    "        entities = self.get_entities(query_str, self.similarity_top_k)\n",
    "\n",
    "        community_ids = self.retrieve_entity_communities(\n",
    "            self.graph_store.entity_info, entities\n",
    "        )\n",
    "        community_summaries = self.graph_store.get_community_summaries()\n",
    "        community_answers = [\n",
    "            self.generate_answer_from_summary(community_summary, query_str)\n",
    "            for id, community_summary in community_summaries.items()\n",
    "            if id in community_ids\n",
    "        ]\n",
    "\n",
    "        final_answer = self.aggregate_answers(community_answers)\n",
    "        return final_answer\n",
    "\n",
    "    def get_entities(self, query_str, similarity_top_k):\n",
    "        nodes_retrieved = self.index.as_retriever(\n",
    "            similarity_top_k=similarity_top_k\n",
    "        ).retrieve(query_str)\n",
    "\n",
    "        enitites = set()\n",
    "        pattern = (\n",
    "            r\"^(\\w+(?:\\s+\\w+)*)\\s*->\\s*([a-zA-Z\\s]+?)\\s*->\\s*(\\w+(?:\\s+\\w+)*)$\"\n",
    "        )\n",
    "\n",
    "        for node in nodes_retrieved:\n",
    "            matches = re.findall(\n",
    "                pattern, node.text, re.MULTILINE | re.IGNORECASE\n",
    "            )\n",
    "\n",
    "            for match in matches:\n",
    "                subject = match[0]\n",
    "                obj = match[2]\n",
    "                enitites.add(subject)\n",
    "                enitites.add(obj)\n",
    "\n",
    "        return list(enitites)\n",
    "\n",
    "    def retrieve_entity_communities(self, entity_info, entities):\n",
    "        \"\"\"\n",
    "        Retrieve cluster information for given entities, allowing for multiple clusters per entity.\n",
    "\n",
    "        Args:\n",
    "        entity_info (dict): Dictionary mapping entities to their cluster IDs (list).\n",
    "        entities (list): List of entity names to retrieve information for.\n",
    "\n",
    "        Returns:\n",
    "        List of community or cluster IDs to which an entity belongs.\n",
    "        \"\"\"\n",
    "        community_ids = []\n",
    "\n",
    "        for entity in entities:\n",
    "            if entity in entity_info:\n",
    "                community_ids.extend(entity_info[entity])\n",
    "\n",
    "        return list(set(community_ids))\n",
    "\n",
    "    def generate_answer_from_summary(self, community_summary, query):\n",
    "        \"\"\"Generate an answer from a community summary based on a given query using LLM.\"\"\"\n",
    "        prompt = (\n",
    "            f\"Given the community summary: {community_summary}, \"\n",
    "            f\"how would you answer the following query? Query: {query}\"\n",
    "        )\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=\"I need an answer based on the above information.\",\n",
    "            ),\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        cleaned_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return cleaned_response\n",
    "\n",
    "    def aggregate_answers(self, community_answers):\n",
    "        \"\"\"Aggregate individual community answers into a final, coherent response.\"\"\"\n",
    "        # intermediate_text = \" \".join(community_answers)\n",
    "        prompt = \"Combine the following intermediate answers into a final, concise response.\"\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=f\"Intermediate answers: {community_answers}\",\n",
    "            ),\n",
    "        ]\n",
    "        final_response = self.llm.chat(messages)\n",
    "        cleaned_final_response = re.sub(\n",
    "            r\"^assistant:\\s*\", \"\", str(final_response)\n",
    "        ).strip()\n",
    "        return cleaned_final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceSplitter\n\u001b[1;32m      3\u001b[0m splitter \u001b[38;5;241m=\u001b[39m SentenceSplitter(\n\u001b[1;32m      4\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      5\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m nodes \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39mget_nodes_from_documents(documents)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/llama_index/core/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Optional\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# response\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Response\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# import global eval handler\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobal_handlers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_global_handler\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/llama_index/core/base/response/schema.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asyncio_run\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbridge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NodeWithScore\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenGen, TokenAsyncGen\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m truncate_text\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/llama_index/core/schema.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbridge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoreSchema\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstrumentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DispatcherSpanMixin\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SAMPLE_TEXT, truncate_text\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Self\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/llama_index/core/utils.py:89\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopwords \u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopwords\n\u001b[0;32m---> 89\u001b[0m globals_helper \u001b[38;5;241m=\u001b[39m \u001b[43mGlobalsHelper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Global Tokenizer\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@runtime_checkable\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenizer\u001b[39;00m(Protocol):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/llama_index/core/utils.py:45\u001b[0m, in \u001b[0;36mGlobalsHelper.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize NLTK stopwords and punkt.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nltk_data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNLTK_DATA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     49\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m         ),\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nltk_data_dir \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/nltk/__init__.py:146\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# PACKAGES\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/nltk/chunk/__init__.py:155\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m     pattern is valid.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkParserI\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnamed_entity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Maxent_NE_Chunker\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregexp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/nltk/chunk/api.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunk parsing API\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m##  Chunk Parser Interface\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m##//////////////////////////////////////////////////////\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkScore\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserI\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/nltk/chunk/util.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy \u001b[38;5;28;01mas\u001b[39;00m _accuracy\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_tag\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m str2tuple\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tree\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/nltk/tag/__init__.py:72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TaggerI\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m str2tuple, tuple2str, untag\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     73\u001b[0m     SequentialBackoffTagger,\n\u001b[1;32m     74\u001b[0m     ContextTagger,\n\u001b[1;32m     75\u001b[0m     DefaultTagger,\n\u001b[1;32m     76\u001b[0m     NgramTagger,\n\u001b[1;32m     77\u001b[0m     UnigramTagger,\n\u001b[1;32m     78\u001b[0m     BigramTagger,\n\u001b[1;32m     79\u001b[0m     TrigramTagger,\n\u001b[1;32m     80\u001b[0m     AffixTagger,\n\u001b[1;32m     81\u001b[0m     RegexpTagger,\n\u001b[1;32m     82\u001b[0m     ClassifierBasedTagger,\n\u001b[1;32m     83\u001b[0m     ClassifierBasedPOSTagger,\n\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrill\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BrillTagger\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrill_trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BrillTaggerTrainer\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/nltk/tag/sequential.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jsontags\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NaiveBayesClassifier\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConditionalFreqDist\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeaturesetTaggerI, TaggerI\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/nltk/classify/__init__.py:97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpositivenaivebayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PositiveNaiveBayesClassifier\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrte_classify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RTEFeatureExtractor, rte_classifier, rte_features\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikitlearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SklearnClassifier\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msenna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Senna\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtextcat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextCat\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/nltk/classify/scikitlearn.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictionaryProbDist\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictVectorizer\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/sklearn/__init__.py:84\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/sklearn/utils/__init__.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/sklearn/utils/_chunking.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/sklearn/utils/validation.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isfinite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FiniteStatus, cy_isfinite\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/sklearn/utils/_array_api.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[1;32m     13\u001b[0m _NUMPY_NAMESPACE_NAMES \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray_api_compat.numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myield_namespaces\u001b[39m(include_numpy_namespaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/sklearn/utils/fixes.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     pd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/pandas/__init__.py:26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/pandas/compat/__init__.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     pa_version_under10p1,\n\u001b[1;32m     29\u001b[0m     pa_version_under11p0,\n\u001b[1;32m     30\u001b[0m     pa_version_under13p0,\n\u001b[1;32m     31\u001b[0m     pa_version_under14p0,\n\u001b[1;32m     32\u001b[0m     pa_version_under14p1,\n\u001b[1;32m     33\u001b[0m     pa_version_under16p0,\n\u001b[1;32m     34\u001b[0m     pa_version_under17p0,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/pandas/compat/pyarrow.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     _palv \u001b[38;5;241m=\u001b[39m Version(Version(pa\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version)\n\u001b[1;32m     11\u001b[0m     pa_version_under10p1 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FAI_FinalProj_Env/lib/python3.11/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ProperGraphIndex using `GraphRAGExtractor` and `GraphRAGStore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_TRIPLET_EXTRACT_TMPL = \"\"\"\n",
    "-Goal-\n",
    "Given a text document, identify all entities and their entity types from the text and all relationships among the identified entities.\n",
    "Given the text, extract up to {max_knowledge_triplets} entity-relation triplets.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: Type of the entity\n",
    "- entity_description: Comprehensive description of the entity's attributes and activities\n",
    "Format each entity as (\"entity\"$$$$<entity_name>$$$$<entity_type>$$$$<entity_description>)\n",
    "\n",
    "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n",
    "For each pair of related entities, extract the following information:\n",
    "- source_entity: name of the source entity, as identified in step 1\n",
    "- target_entity: name of the target entity, as identified in step 1\n",
    "- relation: relationship between source_entity and target_entity\n",
    "- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n",
    "\n",
    "Format each relationship as (\"relationship\"$$$$<source_entity>$$$$<target_entity>$$$$<relation>$$$$<relationship_description>)\n",
    "\n",
    "3. When finished, output.\n",
    "\n",
    "-Real Data-\n",
    "######################\n",
    "text: {text}\n",
    "######################\n",
    "output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pattern = r'\\(\"entity\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\)'\n",
    "relationship_pattern = r'\\(\"relationship\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\$\\$\\$\\$\"(.+?)\"\\)'\n",
    "from llama_index.core.indices.property_graph import DynamicLLMPathExtractor\n",
    "\n",
    "# def parse_fn(response_str: str) -> Any:\n",
    "#     entities = re.findall(entity_pattern, response_str)\n",
    "#     relationships = re.findall(relationship_pattern, response_str)\n",
    "#     print(f\"response_str: {response_str}\")\n",
    "#     print(f\"entities: {entities}\")\n",
    "#     print(f\"relationships: {relationships}\")\n",
    "#     if entities == []:\n",
    "#         entities = [(\"DummyE\", \"DummyE\", \"DummyE\",)]\n",
    "#     if relationships == []:\n",
    "#         relationships = [(\"DummyR\", \"DummyR\", \"DummyR\", \"DummyR\")]\n",
    "#     return entities, relationships\n",
    "\n",
    "def parse_fn(response_str: str) -> Any:\n",
    "    # Updated patterns to match actual output format\n",
    "    entity_pattern = r'\\(\"entity\"\\$\\$\\$\\$(.*?)\\$\\$\\$\\$(.*?)\\$\\$\\$\\$(.*?)\\)'\n",
    "    relationship_pattern = r'\\(\"relationship\"\\$\\$\\$\\$(.*?)\\$\\$\\$\\$(.*?)\\$\\$\\$\\$(.*?)\\$\\$\\$\\$(.*?)\\)'\n",
    "    \n",
    "    # Find all matches\n",
    "    entities = re.findall(entity_pattern, response_str, re.DOTALL)\n",
    "    relationships = re.findall(relationship_pattern, response_str, re.DOTALL)\n",
    "    \n",
    "    # Clean up any whitespace\n",
    "    entities = [(e1.strip(), e2.strip(), e3.strip()) for e1, e2, e3 in entities]\n",
    "    relationships = [(r1.strip(), r2.strip(), r3.strip(), r4.strip()) \n",
    "                    for r1, r2, r3, r4 in relationships]\n",
    "    \n",
    "    # Add default if empty (keeping your original fallback)\n",
    "    if not entities:\n",
    "        entities = [(\"DummyE\", \"DummyE\", \"DummyE\")]\n",
    "    if not relationships:\n",
    "        relationships = [(\"DummyR\", \"DummyR\", \"DummyR\", \"DummyR\")]\n",
    "    \n",
    "    print(f\"Found entities: {entities}\")\n",
    "    print(f\"Found relationships: {relationships}\")\n",
    "    \n",
    "    return entities, relationships\n",
    "\n",
    "kg_extractor = GraphRAGExtractor(\n",
    "    llm=llm,\n",
    "    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,\n",
    "    max_paths_per_chunk=20,\n",
    "    num_workers=4,\n",
    "    parse_fn=parse_fn,\n",
    "\n",
    ")\n",
    "# max_triplets_per_chunk=20,\n",
    "#         num_workers=4\n",
    "# kg_extractor = DynamicLLMPathExtractor(\n",
    "#             llm=llm,\n",
    "#             max_triplets_per_chunk=20,\n",
    "#             num_workers=4,\n",
    "#             allowed_entity_types=None,\n",
    "#             allowed_relation_types=None,\n",
    "#             allowed_relation_props=[\"relationship_description\"],\n",
    "#             allowed_entity_props=[],\n",
    "#             parse_fn=parse_fn,\n",
    "#             extract_prompt=KG_TRIPLET_EXTRACT_TMPL,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Setup And Neo4J setup\n",
    "\n",
    "To launch Neo4j locally, first ensure you have docker installed. Then, you can launch the database with the following docker command.\n",
    "\n",
    "```\n",
    "docker run \\\n",
    "    -p 7474:7474 -p 7687:7687 \\\n",
    "    -v $PWD/data:/data -v $PWD/plugins:/plugins \\\n",
    "    --name neo4j-apoc \\\n",
    "    -e NEO4J_apoc_export_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_use__neo4j__config=true \\\n",
    "    -e NEO4JLABS_PLUGINS=\\[\\\"apoc\\\"\\] \\\n",
    "    neo4j:latest\n",
    "```\n",
    "From here, you can open the db at http://localhost:7474/. On this page, you will be asked to sign in. Use the default username/password of neo4j and neo4j.\n",
    "\n",
    "Once you login for the first time, you will be asked to change the password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "\n",
    "# Note: used to be `Neo4jPGStore`\n",
    "graph_store = GraphRAGStore(\n",
    "    username=\"neo4j\", password=\"password\", url=\"bolt://localhost:7687\", database=database\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text:  20%|██        | 1/5 [00:44<02:57, 44.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_response: ```plaintext\n",
      "(\"entity\"$$$$Latent Relation Mapping Engine$$$$Algorithm$$$$The Latent Relation Mapping Engine is an algorithm designed to automate the process of finding analogical mappings between lists of words using a large corpus of raw text. It combines ideas from Structure Mapping Theory and Latent Relational Analysis, aiming to remove the need for complex hand-coded representations.)\n",
      "\n",
      "(\"entity\"$$$$Structure Mapping Theory (SMT)$$$$Theory$$$$Structure Mapping Theory is a computational model that suggests analogies are based on structural correspondences between two domains. This theory has been implemented in the Structure Mapping Engine (SME).)\n",
      "\n",
      "(\"entity\"$$$$Structure Mapping Engine (SME)$$$$Engine$$$$The Structure Mapping Engine is an implementation of the Structure Mapping Theory, which requires complex hand-coded representations to find analogical mappings.)\n",
      "\n",
      "(\"entity\"$$$$Latent Relational Analysis (LRA)$$$$Analysis Technique$$$$Latent Relational Analysis is a statistical method used to uncover hidden relational structures among variables. It is combined with Structure Mapping Theory in the Latent Relation Mapping Engine (LRME).)\n",
      "\n",
      "(\"relationship\"$$$$Latent Relation Mapping Engine$$$$Structure Mapping Theory (SMT)$$$$Inspired By$$$$The LRME draws inspiration from SMT by incorporating its ideas into a more automated framework.)\n",
      "\n",
      "(\"relationship\"$$$$Latent Relation Mapping Engine$$$$Latent Relational Analysis (LRA)$$$$Integrated With$$$$LRME integrates LRA to automatically discover semantic relations among words without the need for hand-coded representations, much like how SMT is integrated with LRA in LRME.)\n",
      "\n",
      "(\"relationship\"$$$$Structure Mapping Theory (SMT)$$$$Structure Mapping Engine (SME)$$$$Implemented In$$$$SME is an implementation of SMT, which requires complex hand-coded representations.)\n",
      "\n",
      "(\"relationship\"$$$$Latent Relation Mapping Engine$$$$Scientific Analogies$$$$Benchmarked Against$$$$LRME evaluates its performance on problems based on scientific analogies to demonstrate human-level performance.)\n",
      "\n",
      "(\"relationship\"$$$$Latent Relation Mapping Engine$$$$Common Metaphors$$$$Benchmarked Against$$$$LRME also tests its ability to handle common metaphors, another type of analogy problem.)\n",
      "```\n",
      "\n",
      "This output includes 10 entity descriptions and 10 relationship triplets based on the provided text.\n",
      "Found entities: [('Latent Relation Mapping Engine', 'Algorithm', 'The Latent Relation Mapping Engine is an algorithm designed to automate the process of finding analogical mappings between lists of words using a large corpus of raw text. It combines ideas from Structure Mapping Theory and Latent Relational Analysis, aiming to remove the need for complex hand-coded representations.'), ('Structure Mapping Theory (SMT)', 'Theory', 'Structure Mapping Theory is a computational model that suggests analogies are based on structural correspondences between two domains. This theory has been implemented in the Structure Mapping Engine (SME'), ('Structure Mapping Engine (SME)', 'Engine', 'The Structure Mapping Engine is an implementation of the Structure Mapping Theory, which requires complex hand-coded representations to find analogical mappings.'), ('Latent Relational Analysis (LRA)', 'Analysis Technique', 'Latent Relational Analysis is a statistical method used to uncover hidden relational structures among variables. It is combined with Structure Mapping Theory in the Latent Relation Mapping Engine (LRME')]\n",
      "Found relationships: [('Latent Relation Mapping Engine', 'Structure Mapping Theory (SMT)', 'Inspired By', 'The LRME draws inspiration from SMT by incorporating its ideas into a more automated framework.'), ('Latent Relation Mapping Engine', 'Latent Relational Analysis (LRA)', 'Integrated With', 'LRME integrates LRA to automatically discover semantic relations among words without the need for hand-coded representations, much like how SMT is integrated with LRA in LRME.'), ('Structure Mapping Theory (SMT)', 'Structure Mapping Engine (SME)', 'Implemented In', 'SME is an implementation of SMT, which requires complex hand-coded representations.'), ('Latent Relation Mapping Engine', 'Scientific Analogies', 'Benchmarked Against', 'LRME evaluates its performance on problems based on scientific analogies to demonstrate human-level performance.'), ('Latent Relation Mapping Engine', 'Common Metaphors', 'Benchmarked Against', 'LRME also tests its ability to handle common metaphors, another type of analogy problem.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text:  40%|████      | 2/5 [00:48<01:02, 20.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_response: ```plaintext\n",
      "(\"entity\"$$$$Belief Propagation$$$$Algorithm$$$$An iterative algorithm used for inference on tree structures, converging to unique minimum of Bethe free energy. It may not converge or find the global minimum in loopy graphs.)\n",
      "\n",
      "(\"entity\"$$$$Bethe Free Energy Functional$$$$Functional$$$$A functional used in statistical inference on trees that has a unique minimum but can have multiple minima and non-convergent solutions in loopy graphs.)\n",
      "\n",
      "(\"entity\"$$$$Maximum-Likelihood Solution$$$$Solution$$$$The optimal solution for maximum likelihood estimation, often not guaranteed to be found by Belief Propagation in the zero-temperature limit of loopy graphs.)\n",
      "\n",
      "(\"entity\"$$$$Linear Programming (LP) Algorithm$$$$Algorithm$$$$An efficient algorithm used for solving optimization problems with constraints, particularly useful when the matrix of constraints is Totally-Uni-Modular (TUM).)\n",
      "\n",
      "(\"entity\"$$$$Totally-Uni-Modular (TUM) Matrix$$$$Matrix$$$$A special type of matrix that guarantees the LP relaxation problem to have a unique integer solution.)\n",
      "\n",
      "(\"relationship\"$$$$uses$$$$Belief Propagation$$$$Linear Programming Algorithm$$$$Belief Propagation can be generalized by assuming an algorithm that finds the global minimum of Bethe free energy, which could use Linear Programming for optimization in certain models.)\n",
      "\n",
      "(\"relationship\"$$$$relates_to$$$$Bethe Free Energy Functional$$$$Maximum-Likelihood Solution$$$$The Bethe free energy functional's unique minimum is related to the Maximum-Likelihood solution, but this relationship may not hold universally due to multiple minima and non-convergence issues in loopy graphs.)\n",
      "\n",
      "(\"relationship\"$$$$generalizes$$$$Belief Propagation$$$$Linear Programming Algorithm$$$$Belief Propagation can be extended to handle models with loops by using Linear Programming when certain conditions (like TUM constraints) are met.)\n",
      "\n",
      "(\"relationship\"$$$$equivalence_between$$$$Gapless Linear Programming Relaxation$$$$Bethe-Free Energy Minimization$$$$In the limit of zero temperature, the gapless LP relaxation and Bethe-Free energy minimization are equivalent, suggesting that solving one can lead to solutions for the other.)\n",
      "\n",
      "Note: The relationships are inferred based on the context provided in the text. Due to space constraints, only 20 triplets are provided, but additional relevant relationships could exist.\n",
      "```\n",
      "\n",
      "This output identifies key entities and their types from the given text and also provides a few related entity pairs along with explanations for these relationships, adhering to the format specified.\n",
      "Found entities: [('Belief Propagation', 'Algorithm', 'An iterative algorithm used for inference on tree structures, converging to unique minimum of Bethe free energy. It may not converge or find the global minimum in loopy graphs.'), ('Bethe Free Energy Functional', 'Functional', 'A functional used in statistical inference on trees that has a unique minimum but can have multiple minima and non-convergent solutions in loopy graphs.'), ('Maximum-Likelihood Solution', 'Solution', 'The optimal solution for maximum likelihood estimation, often not guaranteed to be found by Belief Propagation in the zero-temperature limit of loopy graphs.'), ('Linear Programming (LP) Algorithm', 'Algorithm', 'An efficient algorithm used for solving optimization problems with constraints, particularly useful when the matrix of constraints is Totally-Uni-Modular (TUM'), ('Totally-Uni-Modular (TUM) Matrix', 'Matrix', 'A special type of matrix that guarantees the LP relaxation problem to have a unique integer solution.')]\n",
      "Found relationships: [('uses', 'Belief Propagation', 'Linear Programming Algorithm', 'Belief Propagation can be generalized by assuming an algorithm that finds the global minimum of Bethe free energy, which could use Linear Programming for optimization in certain models.'), ('relates_to', 'Bethe Free Energy Functional', 'Maximum-Likelihood Solution', \"The Bethe free energy functional's unique minimum is related to the Maximum-Likelihood solution, but this relationship may not hold universally due to multiple minima and non-convergence issues in loopy graphs.\"), ('generalizes', 'Belief Propagation', 'Linear Programming Algorithm', 'Belief Propagation can be extended to handle models with loops by using Linear Programming when certain conditions (like TUM constraints'), ('equivalence_between', 'Gapless Linear Programming Relaxation', 'Bethe-Free Energy Minimization', 'In the limit of zero temperature, the gapless LP relaxation and Bethe-Free energy minimization are equivalent, suggesting that solving one can lead to solutions for the other.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text:  60%|██████    | 3/5 [01:31<01:01, 30.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_response: ### Entities Identified:\n",
      "1. (\"entity\"$$$$Corner Detector$$$$Type$$$$Determines how likely it is to be useful in real-world applications; involves repeatability and efficiency.\")\n",
      "2. (\"entity\"$$$$Repeatability$$$$Metric$$$$Measure of consistency across different viewing positions of the same scene.)\n",
      "3. (\"entity\"$$$$Efficiency$$$$Metric$$$$Determine whether the detector can operate at frame rate with further processing.)\n",
      "4. (\"entity\"$$$$Feature Detection$$$$Process$$$$The process of identifying corners in an image or video.)\n",
      "5. (\"entity\"$$$$Live PAL Video$$$$Input$$$$Video input processed by the feature detector for real-time application.)\n",
      "6. (\"entity\"$$$$Harris Detector$$$$Existing Technology$$$$A corner detection algorithm that operates slower than the new heuristic detector.\")\n",
      "7. (\"entity\"$$$$SIFT (Scale-Invariant Feature Transform)$$$$Existing Technology$$$$Another slow corner detection algorithm compared with the new heuristic detector.\")\n",
      "8. (\"entity\"$$$$Machine Learning$$$$Technique$$$$Used to derive a feature detector from a new heuristic for real-time processing of live video.)\n",
      "9. (\"entity\"$$$$3D Scenes$$$$Test Scenario$$$$Scenes used in rigorous comparisons of different corner detectors based on repeatability.)\n",
      "\n",
      "### Relationships Identified:\n",
      "1. (\"relationship\"$$$$Corner Detector$$$$Repeatability$$$$Determines usefulness$$$$The consistency of the detector's performance across different viewing positions is crucial for its real-world utility.)\n",
      "2. (\"relationship\"$$$$Corner Detector$$$$Efficiency$$$$Determines frame rate operation$$$$The efficiency affects whether the detector can operate at a high frame rate with additional processing.)\n",
      "3. (\"relationship\"$$$$Repeatability$$$$Efficiency$$$$Trade-off$$$$While repeatability ensures consistent results, it must not come at the cost of efficiency to ensure real-time operation.)\n",
      "4. (\"relationship\"$$$$Feature Detection$$$$Live PAL Video$$$$Input Source$$$$The feature detector processes live PAL video in real time.)\n",
      "5. (\"relationship\"$$$$Machine Learning$$$$Feature Detection$$$$Methodology$$$$Machine learning is used as a methodology to develop the new feature detector from heuristic methods.)\n",
      "6. (\"relationship\"$$$$Harris Detector$$$$Existing Technology$$$$Comparison Benchmark$$$$Compared against the new heuristic detector for its slower performance on live video processing.)\n",
      "7. (\"relationship\"$$$$SIFT$$$$Existing Technology$$$$Comparison Benchmark$$$$Compared with the new detector for its slower performance in real-time applications.)\n",
      "8. (\"relationship\"$$$$Repeatability Criterion$$$$3D Scenes$$$$Test Scenario$$$$The repeatability criterion is applied to 3D scenes during rigorous testing of corner detectors.)\n",
      "9. (\"relationship\"$$$$Machine Learning Technique$$$$Repeatability Improvement$$$$Effectiveness$$$$Using machine learning leads to significant improvements in repeatability, making the detector both fast and high quality.)\n",
      "\n",
      "### Output:\n",
      "```\n",
      "(\"entity\"$$$$Corner Detector$$$$Type$$$$Determines how likely it is to be useful in real-world applications; involves repeatability and efficiency.)\n",
      "(\"entity\"$$$$Repeatability$$$$Metric$$$$Measure of consistency across different viewing positions of the same scene.)\n",
      "(\"entity\"$$$$Efficiency$$$$Metric$$$$Determine whether the detector can operate at frame rate with further processing.)\n",
      "(\"entity\"$$$$Feature Detection$$$$Process$$$$The process of identifying corners in an image or video.)\n",
      "(\"entity\"$$$$Live PAL Video$$$$Input$$$$Video input processed by the feature detector for real-time application.)\n",
      "(\"entity\"$$$$Harris Detector$$$$Existing Technology$$$$A corner detection algorithm that operates slower than the new heuristic detector.)\n",
      "(\"entity\"$$$$SIFT (Scale-Invariant Feature Transform)$$$$Existing Technology$$$$Another slow corner detection algorithm compared with the new heuristic detector.)\n",
      "(\"entity\"$$$$Machine Learning$$$$Technique$$$$Used to derive a feature detector from a new heuristic for real-time processing of live video.)\n",
      "(\"entity\"$$$$3D Scenes$$$$Test Scenario$$$$Scenes used in rigorous comparisons of different corner detectors based on repeatability criterion applied to 3D scenes.)\n",
      "\n",
      "(\"relationship\"$$$$Corner Detector$$$$Repeatability$$$$Determines usefulness$$$$The consistency of the detector's performance across different viewing positions is crucial for its real-world utility.)\n",
      "(\"relationship\"$$$$Corner Detector$$$$Efficiency$$$$Determines frame rate operation$$$$The efficiency affects whether the detector can operate at a high frame rate with additional processing.)\n",
      "(\"relationship\"$$$$Repeatability$$$$Efficiency$$$$Trade-off$$$$While repeatability ensures consistent results, it must not come at the cost of efficiency to ensure real-time operation.)\n",
      "(\"relationship\"$$$$Feature Detection$$$$Live PAL Video$$$$Input Source$$$$The feature detector processes live PAL video in real time.)\n",
      "(\"relationship\"$$$$Machine Learning$$$$Feature Detection$$$$Methodology$$$$Machine learning is used as a methodology to develop the new feature detector from heuristic methods.)\n",
      "(\"relationship\"$$$$Harris Detector$$$$Existing Technology$$$$Comparison Benchmark$$$$Compared against the new heuristic detector for its slower performance on live video processing.)\n",
      "(\"relationship\"$$$$SIFT$$$$Existing Technology$$$$Comparison Benchmark$$$$Compared with the new detector for its slower performance in real-time applications.)\n",
      "(\"relationship\"$$$$Repeatability Criterion$$$$3D Scenes$$$$Test Scenario$$$$The repeatability criterion is applied to 3D scenes during rigorous testing of corner detectors.)\n",
      "(\"relationship\"$$$$Machine Learning Technique$$$$Repeatability Improvement$$$$Effectiveness$$$$Using machine learning leads to significant improvements in repeatability, making the detector both fast and high quality.)\n",
      "```\n",
      "Found entities: [('Corner Detector', 'Type', 'Determines how likely it is to be useful in real-world applications; involves repeatability and efficiency.\"'), ('Repeatability', 'Metric', 'Measure of consistency across different viewing positions of the same scene.'), ('Efficiency', 'Metric', 'Determine whether the detector can operate at frame rate with further processing.'), ('Feature Detection', 'Process', 'The process of identifying corners in an image or video.'), ('Live PAL Video', 'Input', 'Video input processed by the feature detector for real-time application.'), ('Harris Detector', 'Existing Technology', 'A corner detection algorithm that operates slower than the new heuristic detector.\"'), ('SIFT (Scale-Invariant Feature Transform)', 'Existing Technology', 'Another slow corner detection algorithm compared with the new heuristic detector.\"'), ('Machine Learning', 'Technique', 'Used to derive a feature detector from a new heuristic for real-time processing of live video.'), ('3D Scenes', 'Test Scenario', 'Scenes used in rigorous comparisons of different corner detectors based on repeatability.'), ('Corner Detector', 'Type', 'Determines how likely it is to be useful in real-world applications; involves repeatability and efficiency.'), ('Repeatability', 'Metric', 'Measure of consistency across different viewing positions of the same scene.'), ('Efficiency', 'Metric', 'Determine whether the detector can operate at frame rate with further processing.'), ('Feature Detection', 'Process', 'The process of identifying corners in an image or video.'), ('Live PAL Video', 'Input', 'Video input processed by the feature detector for real-time application.'), ('Harris Detector', 'Existing Technology', 'A corner detection algorithm that operates slower than the new heuristic detector.'), ('SIFT (Scale-Invariant Feature Transform)', 'Existing Technology', 'Another slow corner detection algorithm compared with the new heuristic detector.'), ('Machine Learning', 'Technique', 'Used to derive a feature detector from a new heuristic for real-time processing of live video.'), ('3D Scenes', 'Test Scenario', 'Scenes used in rigorous comparisons of different corner detectors based on repeatability criterion applied to 3D scenes.')]\n",
      "Found relationships: [('Corner Detector', 'Repeatability', 'Determines usefulness', \"The consistency of the detector's performance across different viewing positions is crucial for its real-world utility.\"), ('Corner Detector', 'Efficiency', 'Determines frame rate operation', 'The efficiency affects whether the detector can operate at a high frame rate with additional processing.'), ('Repeatability', 'Efficiency', 'Trade-off', 'While repeatability ensures consistent results, it must not come at the cost of efficiency to ensure real-time operation.'), ('Feature Detection', 'Live PAL Video', 'Input Source', 'The feature detector processes live PAL video in real time.'), ('Machine Learning', 'Feature Detection', 'Methodology', 'Machine learning is used as a methodology to develop the new feature detector from heuristic methods.'), ('Harris Detector', 'Existing Technology', 'Comparison Benchmark', 'Compared against the new heuristic detector for its slower performance on live video processing.'), ('SIFT', 'Existing Technology', 'Comparison Benchmark', 'Compared with the new detector for its slower performance in real-time applications.'), ('Repeatability Criterion', '3D Scenes', 'Test Scenario', 'The repeatability criterion is applied to 3D scenes during rigorous testing of corner detectors.'), ('Machine Learning Technique', 'Repeatability Improvement', 'Effectiveness', 'Using machine learning leads to significant improvements in repeatability, making the detector both fast and high quality.'), ('Corner Detector', 'Repeatability', 'Determines usefulness', \"The consistency of the detector's performance across different viewing positions is crucial for its real-world utility.\"), ('Corner Detector', 'Efficiency', 'Determines frame rate operation', 'The efficiency affects whether the detector can operate at a high frame rate with additional processing.'), ('Repeatability', 'Efficiency', 'Trade-off', 'While repeatability ensures consistent results, it must not come at the cost of efficiency to ensure real-time operation.'), ('Feature Detection', 'Live PAL Video', 'Input Source', 'The feature detector processes live PAL video in real time.'), ('Machine Learning', 'Feature Detection', 'Methodology', 'Machine learning is used as a methodology to develop the new feature detector from heuristic methods.'), ('Harris Detector', 'Existing Technology', 'Comparison Benchmark', 'Compared against the new heuristic detector for its slower performance on live video processing.'), ('SIFT', 'Existing Technology', 'Comparison Benchmark', 'Compared with the new detector for its slower performance in real-time applications.'), ('Repeatability Criterion', '3D Scenes', 'Test Scenario', 'The repeatability criterion is applied to 3D scenes during rigorous testing of corner detectors.'), ('Machine Learning Technique', 'Repeatability Improvement', 'Effectiveness', 'Using machine learning leads to significant improvements in repeatability, making the detector both fast and high quality.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text:  80%|████████  | 4/5 [01:33<00:19, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_response: Based on the given text, let's extract and identify entities, their types, descriptions, relations, and detailed explanations.\n",
      "\n",
      "### Step 1: Identifying Entities\n",
      "\n",
      "1. **Garg**\n",
      "   - entity_name: Garg\n",
      "   - entity_type: Person\n",
      "   - entity_description: A researcher or co-author involved in the work related to access control logics.\n",
      "\n",
      "2. **Abadi**\n",
      "   - entity_name: Abadi\n",
      "   - entity_type: Person\n",
      "   - entity_description: A researcher or co-author involved in the work related to access control logics.\n",
      "\n",
      "3. **Automating Access Control Logics in Simple Type Theory with LEO-II** (This is a title, not an entity)\n",
      "4. **LEO-II**\n",
      "   - entity_name: LEO-II\n",
      "   - entity_type: Software\n",
      "   - entity_description: A higher-order theorem prover used for automated reasoning.\n",
      "\n",
      "5. **Modal logic S4**\n",
      "   - entity_name: Modal logic S4\n",
      "   - entity_type: Logical System\n",
      "   - entity_description: A modal logic system known for its sound and complete translations of access control logics.\n",
      "\n",
      "6. **Simple Type Theory** (This is a title, not an entity)\n",
      "7. **Normal multimodal logics**\n",
      "   - entity_name: Normal Multimodal Logics\n",
      "   - entity_type: Logical System\n",
      "   - entity_description: A class of modal logics that includes monomodal systems K and S4.\n",
      "\n",
      "8. **Monomodal logic K**\n",
      "   - entity_name: Monomodal Logic K\n",
      "   - entity_type: Logical System\n",
      "   - entity_description: A foundational system within the broader class of normal multimodal logics.\n",
      "\n",
      "9. **Higher-order logic** (This is a title, not an entity)\n",
      "10. **Theorem Prover**\n",
      "    - entity_name: Theorem Prover\n",
      "    - entity_type: Software Tool\n",
      "    - entity_description: A software tool designed for proving theorems in formal systems.\n",
      "\n",
      "### Step 2: Identifying Relationships\n",
      "\n",
      "1. **(\"relationship\"$$$$Garg$$$$Abadi$$$$Co-Authorship$$$$Both are co-authors involved in the work related to access control logics.)**\n",
      "\n",
      "2. **(\"relationship\"$$$$Garg$$$$LEO-II$$$$Research Tool$$$$(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.)$$)**\n",
      "\n",
      "3. **(\"relationship\"$$$$Abadi$$$$LEO-II$$$$Research Tool$$$$(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.)$$)**\n",
      "\n",
      "4. **(\"relationship\"$$$$Modal logic S4$$$$Normal Multimodal Logics$$$$Subsumption$$$$(Modal logic S4 is a part of the normal multimodal logics class, including monomodal logic K and other systems.)$$)**\n",
      "\n",
      "5. **(\"relationship\"$$$$Normal Multimodal Logics$$$$Higher-order logic$$$$Embedding$$$$(The normal multimodal logics can be embedded within higher-order logic as demonstrated by previous work.)$$)**\n",
      "\n",
      "6. **(\"relationship\"$$$$LEO-II$$$$Automating Reasoning in Access Control Logics$$$$Application$$$$(LEO-II is applied to automate reasoning in prominent access control logics as per the research described in this paper.)$$)**\n",
      "\n",
      "### Final Output:\n",
      "\n",
      "- (\"entity\"$$$$Garg$$$$Person$$$$A researcher or co-author involved in the work related to access control logics.)\n",
      "- (\"entity\"$$$$Abadi$$$$Person$$$$A researcher or co-author involved in the work related to access control logics.)\n",
      "- (\"entity\"$$$$LEO-II$$$$Software$$$$A higher-order theorem prover used for automated reasoning.)\n",
      "- (\"entity\"$$$$Modal logic S4$$$$Logical System$$$$A modal logic system known for its sound and complete translations of access control logics.)\n",
      "- (\"entity\"$$$$Normal Multimodal Logics$$$$Logical System$$$$A class of modal logics that includes monomodal systems K and other systems.)\n",
      "- (\"entity\"$$$$Monomodal Logic K$$$$Logical System$$$$A foundational system within the broader class of normal multimodal logics.)\n",
      "- (\"entity\"$$$$Higher-order logic$$$$Logical System$$$$A formal system used for embedding various logical systems like normal multimodal logics.)\n",
      "\n",
      "- (\"relationship\"$$$$Garg$$$$Abadi$$$$Co-Authorship$$$$Both are co-authors involved in the work related to access control logics.)\n",
      "- (\"relationship\"$$$$Garg$$$$LEO-II$$$$Research Tool$$$$(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.)$$)\n",
      "- (\"relationship\"$$$$Abadi$$$$LEO-II$$$$Research Tool$$$$(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.)$$)\n",
      "- (\"relationship\"$$$$Modal logic S4$$$$Normal Multimodal Logics$$$$Subsumption$$$$(Modal logic S4 is a part of the normal multimodal logics class, including monomodal logic K and other systems.)$$)\n",
      "- (\"relationship\"$$$$Normal Multimodal Logics$$$$Higher-order logic$$$$Embedding$$$$(The normal multimodal logics can be embedded within higher-order logic as demonstrated by previous work.)$$)\n",
      "- (\"relationship\"$$$$LEO-II$$$$Automating Reasoning in Access Control Logics$$$$Application$$$$(LEO-II is applied to automate reasoning in prominent access control logics as per the research described in this paper.)$$)\n",
      "Found entities: [('Garg', 'Person', 'A researcher or co-author involved in the work related to access control logics.'), ('Abadi', 'Person', 'A researcher or co-author involved in the work related to access control logics.'), ('LEO-II', 'Software', 'A higher-order theorem prover used for automated reasoning.'), ('Modal logic S4', 'Logical System', 'A modal logic system known for its sound and complete translations of access control logics.'), ('Normal Multimodal Logics', 'Logical System', 'A class of modal logics that includes monomodal systems K and other systems.'), ('Monomodal Logic K', 'Logical System', 'A foundational system within the broader class of normal multimodal logics.'), ('Higher-order logic', 'Logical System', 'A formal system used for embedding various logical systems like normal multimodal logics.')]\n",
      "Found relationships: [('Garg', 'Abadi', 'Co-Authorship', 'Both are co-authors involved in the work related to access control logics.'), ('Garg', 'LEO-II', 'Research Tool', '(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.'), ('Abadi', 'LEO-II', 'Research Tool', '(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.'), ('Modal logic S4', 'Normal Multimodal Logics', 'Subsumption', '(Modal logic S4 is a part of the normal multimodal logics class, including monomodal logic K and other systems.'), ('Normal Multimodal Logics', 'Higher-order logic', 'Embedding', '(The normal multimodal logics can be embedded within higher-order logic as demonstrated by previous work.'), ('LEO-II', 'Automating Reasoning in Access Control Logics', 'Application', '(LEO-II is applied to automate reasoning in prominent access control logics as per the research described in this paper.'), ('Garg', 'Abadi', 'Co-Authorship', 'Both are co-authors involved in the work related to access control logics.'), ('Garg', 'LEO-II', 'Research Tool', '(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.'), ('Abadi', 'LEO-II', 'Research Tool', '(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.'), ('Modal logic S4', 'Normal Multimodal Logics', 'Subsumption', '(Modal logic S4 is a part of the normal multimodal logics class, including monomodal logic K and other systems.'), ('Normal Multimodal Logics', 'Higher-order logic', 'Embedding', '(The normal multimodal logics can be embedded within higher-order logic as demonstrated by previous work.'), ('LEO-II', 'Automating Reasoning in Access Control Logics', 'Application', '(LEO-II is applied to automate reasoning in prominent access control logics as per the research described in this paper.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text: 100%|██████████| 5/5 [01:53<00:00, 22.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_response: ### Step 1: Identify all entities\n",
      "\n",
      "- Grammar-Based Random Walkers: $$$$Grammar-Based Random Walkers$$$$Random Walkers$$$$A model used in semantic networks to rank vertices based on a modified random walker constrained by a user-defined grammar.\n",
      "- Semantic Networks: $$$$Semantic Networks$$$$Networks$$$$Collections of interconnected concepts or nodes where edges qualify the relationships between these nodes. The text mentions that vertices can be \"central\" and relationships are context-dependent.\n",
      "- Vertices: $$$$Vertices$$$$Nodes$$$$The basic elements in semantic networks, representing concepts or entities. Their centrality is subjective and depends on the relationship type.\n",
      "- Context-Based Rankings: $$$$Context-Based Rankings$$$$Rankings$$$$Metrics based on user-defined contexts that rank vertices in a semantic network. The text highlights their importance over direct vertex rankings.\n",
      "- Eigenvector Centrality: $$$$Eigenvector Centrality$$$$Metric$$$$A measure of the influence of a node in a network, where its value is determined by the values of the neighboring nodes.\n",
      "- PageRank: $$$$PageRank$$$$Metric$$$$Similar to eigenvector centrality, it ranks vertices based on their importance or influence within a network. The text notes that both metrics are used in semantic networks.\n",
      "- Random Walkers (Markov Chain Analysis): $$$$Random Walkers (Markov Chain Analysis)$$$$Model$$$$A probabilistic model where the probability of moving from one vertex to another is defined by transition probabilities, modified here by a user-defined grammar.\n",
      "- Resource Description Framework (RDF): $$$$Resource Description Framework (RDF)$$$$Framework$$$$A standard for processing and sharing metadata in the Semantic Web. It's used as an example context within which semantic network ideas are applied.\n",
      "\n",
      "### Step 2: Identify relationships\n",
      "\n",
      "#### Relationship 1\n",
      "- source_entity: Grammar-Based Random Walkers\n",
      "- target_entity: Markov Chain Analysis\n",
      "- relation: Based On\n",
      "- relationship_description: The text explicitly states that the random walker model is a modified version of the random walker model used in Markov chain analysis.\n",
      "\n",
      "#### Relationship 2\n",
      "- source_entity: Semantic Networks\n",
      "- target_entity: Vertices\n",
      "- relation: Contains\n",
      "- relationship_description: Semantic networks are described as collections of interconnected concepts or nodes, with vertices being one such element.\n",
      "\n",
      "#### Relationship 3\n",
      "- source_entity: Context-Based Rankings\n",
      "- target_entity: User Defined Contexts\n",
      "- relation: Dependent On\n",
      "- relationship_description: The text mentions that context-based rankings are based on user-defined contexts, indicating a dependency between the two entities.\n",
      "\n",
      "#### Relationship 4\n",
      "- source_entity: Eigenvector Centrality\n",
      "- target_entity: Vertices\n",
      "- relation: Measures Influence Of\n",
      "- relationship_description: Eigenvector centrality is defined as a measure of influence for vertices in a network.\n",
      "\n",
      "#### Relationship 5\n",
      "- source_entity: PageRank\n",
      "- target_entity: Vertices\n",
      "- relation: Measures Influence Of\n",
      "- relationship_description: Similar to eigenvector centrality, PageRank also ranks the importance or influence of vertices within a network.\n",
      "\n",
      "#### Relationship 6\n",
      "- source_entity: Grammar-Based Random Walkers\n",
      "- target_entity: User Defined Grammar\n",
      "- relation: Constrained By\n",
      "- relationship_description: The text specifies that random walkers are constrained by a user-defined grammar, which determines their meaning and behavior in the semantic network.\n",
      "\n",
      "### Step 3: Output\n",
      "\n",
      "(\"entity\"$$$$Grammar-Based Random Walkers$$$$Random Walkers$$$$A model used in semantic networks to rank vertices based on a modified random walker constrained by a user-defined grammar.)\n",
      "(\"entity\"$$$$Semantic Networks$$$$Networks$$$$Collections of interconnected concepts or nodes where edges qualify the relationships between these nodes. The text mentions that vertices can be \"central\" and relationships are context-dependent.)\n",
      "(\"entity\"$$$$Vertices$$$$Nodes$$$$The basic elements in semantic networks, representing concepts or entities. Their centrality is subjective and depends on the relationship type.)\n",
      "(\"entity\"$$$$Context-Based Rankings$$$$Rankings$$$$Metrics based on user-defined contexts that rank vertices in a semantic network. The text highlights their importance over direct vertex rankings.)\n",
      "(\"entity\"$$$$Eigenvector Centrality$$$$Metric$$$$A measure of the influence of a node in a network, where its value is determined by the values of the neighboring nodes.)\n",
      "(\"entity\"$$$$PageRank$$$$Metric$$$$Similar to eigenvector centrality, it ranks vertices based on their importance or influence within a network. The text notes that both metrics are used in semantic networks.)\n",
      "(\"entity\"$$$$Random Walkers (Markov Chain Analysis)$$$$Model$$$$A probabilistic model where the probability of moving from one vertex to another is defined by transition probabilities, modified here by a user-defined grammar.)\n",
      "(\"entity\"$$$$Resource Description Framework (RDF)$$$$Framework$$$$A standard for processing and sharing metadata in the Semantic Web. It's used as an example context within which semantic network ideas are applied.)\n",
      "\n",
      "(\"relationship\"$$$$Grammar-Based Random Walkers$$$$Random Walkers (Markov Chain Analysis)$$$$Based On$$$$The text explicitly states that the random walker model is a modified version of the random walker model used in Markov chain analysis.)\n",
      "(\"relationship\"$$$$Semantic Networks$$$$Vertices$$$$Contains$$$$Semantic networks are described as collections of interconnected concepts or nodes, with vertices being one such element.)\n",
      "(\"relationship\"$$$$Context-Based Rankings$$$$User Defined Contexts$$$$Dependent On$$$$The text mentions that context-based rankings are based on user-defined contexts, indicating a dependency between the two entities.)\n",
      "(\"relationship\"$$$$Eigenvector Centrality$$$$Vertices$$$$Measures Influence Of$$$$Eigenvector centrality is defined as a measure of influence for vertices in a network.)\n",
      "(\"relationship\"$$$$PageRank$$$$Vertices$$$$Measures Influence Of$$$$Similar to eigenvector centrality, PageRank also ranks the importance or influence of vertices within a network.)\n",
      "(\"relationship\"$$$$Grammar-Based Random Walkers$$$$User Defined Grammar$$$$Constrained By$$$$The text specifies that random walkers are constrained by a user-defined grammar, which determines their meaning and behavior in the semantic network.)\n",
      "Found entities: [('Grammar-Based Random Walkers', 'Random Walkers', 'A model used in semantic networks to rank vertices based on a modified random walker constrained by a user-defined grammar.'), ('Semantic Networks', 'Networks', 'Collections of interconnected concepts or nodes where edges qualify the relationships between these nodes. The text mentions that vertices can be \"central\" and relationships are context-dependent.'), ('Vertices', 'Nodes', 'The basic elements in semantic networks, representing concepts or entities. Their centrality is subjective and depends on the relationship type.'), ('Context-Based Rankings', 'Rankings', 'Metrics based on user-defined contexts that rank vertices in a semantic network. The text highlights their importance over direct vertex rankings.'), ('Eigenvector Centrality', 'Metric', 'A measure of the influence of a node in a network, where its value is determined by the values of the neighboring nodes.'), ('PageRank', 'Metric', 'Similar to eigenvector centrality, it ranks vertices based on their importance or influence within a network. The text notes that both metrics are used in semantic networks.'), ('Random Walkers (Markov Chain Analysis)', 'Model', 'A probabilistic model where the probability of moving from one vertex to another is defined by transition probabilities, modified here by a user-defined grammar.'), ('Resource Description Framework (RDF)', 'Framework', \"A standard for processing and sharing metadata in the Semantic Web. It's used as an example context within which semantic network ideas are applied.\")]\n",
      "Found relationships: [('Grammar-Based Random Walkers', 'Random Walkers (Markov Chain Analysis)', 'Based On', 'The text explicitly states that the random walker model is a modified version of the random walker model used in Markov chain analysis.'), ('Semantic Networks', 'Vertices', 'Contains', 'Semantic networks are described as collections of interconnected concepts or nodes, with vertices being one such element.'), ('Context-Based Rankings', 'User Defined Contexts', 'Dependent On', 'The text mentions that context-based rankings are based on user-defined contexts, indicating a dependency between the two entities.'), ('Eigenvector Centrality', 'Vertices', 'Measures Influence Of', 'Eigenvector centrality is defined as a measure of influence for vertices in a network.'), ('PageRank', 'Vertices', 'Measures Influence Of', 'Similar to eigenvector centrality, PageRank also ranks the importance or influence of vertices within a network.'), ('Grammar-Based Random Walkers', 'User Defined Grammar', 'Constrained By', 'The text specifies that random walkers are constrained by a user-defined grammar, which determines their meaning and behavior in the semantic network.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:01<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# embed_model = HuggingFaceEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embed_model = HuggingFaceEmbedding(\"avsolatorio/GIST-all-MiniLM-L6-v2\")\n",
    "# GIST-all-MiniLM-L6-v2 \n",
    "# PropertyGraphIndex.from_documents(\n",
    "#             documents,\n",
    "#             property_graph_store=graph_store,\n",
    "#             llm=self.llm,\n",
    "#             embed_model=self.embed_model,\n",
    "#             embed_kg_nodes=True,\n",
    "#             kg_extractors=[self.kg_extractor],\n",
    "#             show_progress=True\n",
    "# )\n",
    "\n",
    "index = PropertyGraphIndex(\n",
    "    nodes=nodes,\n",
    "    kg_extractors=[kg_extractor],\n",
    "    property_graph_store=graph_store,\n",
    "    llm=llm,\n",
    "    show_progress=True,\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[EntityNode(label='Random Walkers', embedding=None, properties={'id': 'Grammar-Based Random Walkers', 'entity_description': 'A model used in semantic networks to rank vertices based on a modified random walker constrained by a user-defined grammar.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Grammar-Based Random Walkers'),\n",
       "  Relation(label='Based On', source_id='Grammar-Based Random Walkers', target_id='Random Walkers (Markov Chain Analysis)', properties={'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a', 'relationship_description': 'The text explicitly states that the random walker model is a modified version of the random walker model used in Markov chain analysis.'}),\n",
       "  EntityNode(label='Model', embedding=None, properties={'id': 'Random Walkers (Markov Chain Analysis)', 'entity_description': 'A probabilistic model where the probability of moving from one vertex to another is defined by transition probabilities, modified here by a user-defined grammar.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Random Walkers (Markov Chain Analysis)')],\n",
       " [EntityNode(label='Networks', embedding=None, properties={'id': 'Semantic Networks', 'entity_description': 'Collections of interconnected concepts or nodes where edges qualify the relationships between these nodes. The text mentions that vertices can be \"central\" and relationships are context-dependent.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Semantic Networks'),\n",
       "  Relation(label='Contains', source_id='Semantic Networks', target_id='Vertices', properties={'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a', 'relationship_description': 'Semantic networks are described as collections of interconnected concepts or nodes, with vertices being one such element.'}),\n",
       "  EntityNode(label='Nodes', embedding=None, properties={'id': 'Vertices', 'entity_description': 'The basic elements in semantic networks, representing concepts or entities. Their centrality is subjective and depends on the relationship type.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Vertices')],\n",
       " [EntityNode(label='Metric', embedding=None, properties={'id': 'Eigenvector Centrality', 'entity_description': 'A measure of the influence of a node in a network, where its value is determined by the values of the neighboring nodes.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Eigenvector Centrality'),\n",
       "  Relation(label='Measures Influence Of', source_id='Eigenvector Centrality', target_id='Vertices', properties={'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a', 'relationship_description': 'Eigenvector centrality is defined as a measure of influence for vertices in a network.'}),\n",
       "  EntityNode(label='Nodes', embedding=None, properties={'id': 'Vertices', 'entity_description': 'The basic elements in semantic networks, representing concepts or entities. Their centrality is subjective and depends on the relationship type.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Vertices')],\n",
       " [EntityNode(label='Metric', embedding=None, properties={'id': 'PageRank', 'entity_description': 'Similar to eigenvector centrality, it ranks vertices based on their importance or influence within a network. The text notes that both metrics are used in semantic networks.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='PageRank'),\n",
       "  Relation(label='Measures Influence Of', source_id='PageRank', target_id='Vertices', properties={'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a', 'relationship_description': 'Similar to eigenvector centrality, PageRank also ranks the importance or influence of vertices within a network.'}),\n",
       "  EntityNode(label='Nodes', embedding=None, properties={'id': 'Vertices', 'entity_description': 'The basic elements in semantic networks, representing concepts or entities. Their centrality is subjective and depends on the relationship type.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Vertices')],\n",
       " [EntityNode(label='Type', embedding=None, properties={'id': 'Corner Detector', 'entity_description': 'Determines how likely it is to be useful in real-world applications; involves repeatability and efficiency.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Corner Detector'),\n",
       "  Relation(label='Determines usefulness', source_id='Corner Detector', target_id='Repeatability', properties={'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7', 'relationship_description': \"The consistency of the detector's performance across different viewing positions is crucial for its real-world utility.\"}),\n",
       "  EntityNode(label='Metric', embedding=None, properties={'id': 'Repeatability', 'entity_description': 'Measure of consistency across different viewing positions of the same scene.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Repeatability')],\n",
       " [EntityNode(label='Type', embedding=None, properties={'id': 'Corner Detector', 'entity_description': 'Determines how likely it is to be useful in real-world applications; involves repeatability and efficiency.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Corner Detector'),\n",
       "  Relation(label='Determines frame rate operation', source_id='Corner Detector', target_id='Efficiency', properties={'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7', 'relationship_description': 'The efficiency affects whether the detector can operate at a high frame rate with additional processing.'}),\n",
       "  EntityNode(label='Metric', embedding=None, properties={'id': 'Efficiency', 'entity_description': 'Determine whether the detector can operate at frame rate with further processing.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Efficiency')],\n",
       " [EntityNode(label='Metric', embedding=None, properties={'id': 'Repeatability', 'entity_description': 'Measure of consistency across different viewing positions of the same scene.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Repeatability'),\n",
       "  Relation(label='Trade-off', source_id='Repeatability', target_id='Efficiency', properties={'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7', 'relationship_description': 'While repeatability ensures consistent results, it must not come at the cost of efficiency to ensure real-time operation.'}),\n",
       "  EntityNode(label='Metric', embedding=None, properties={'id': 'Efficiency', 'entity_description': 'Determine whether the detector can operate at frame rate with further processing.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Efficiency')],\n",
       " [EntityNode(label='Process', embedding=None, properties={'id': 'Feature Detection', 'entity_description': 'The process of identifying corners in an image or video.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Feature Detection'),\n",
       "  Relation(label='Input Source', source_id='Feature Detection', target_id='Live PAL Video', properties={'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7', 'relationship_description': 'The feature detector processes live PAL video in real time.'}),\n",
       "  EntityNode(label='Input', embedding=None, properties={'id': 'Live PAL Video', 'entity_description': 'Video input processed by the feature detector for real-time application.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Live PAL Video')],\n",
       " [EntityNode(label='Technique', embedding=None, properties={'id': 'Machine Learning', 'entity_description': 'Used to derive a feature detector from a new heuristic for real-time processing of live video.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Machine Learning'),\n",
       "  Relation(label='Methodology', source_id='Machine Learning', target_id='Feature Detection', properties={'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7', 'relationship_description': 'Machine learning is used as a methodology to develop the new feature detector from heuristic methods.'}),\n",
       "  EntityNode(label='Process', embedding=None, properties={'id': 'Feature Detection', 'entity_description': 'The process of identifying corners in an image or video.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Feature Detection')],\n",
       " [EntityNode(label='Algorithm', embedding=None, properties={'id': 'Latent Relation Mapping Engine', 'entity_description': 'The Latent Relation Mapping Engine is an algorithm designed to automate the process of finding analogical mappings between lists of words using a large corpus of raw text. It combines ideas from Structure Mapping Theory and Latent Relational Analysis, aiming to remove the need for complex hand-coded representations.', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Latent Relation Mapping Engine'),\n",
       "  Relation(label='Inspired By', source_id='Latent Relation Mapping Engine', target_id='Structure Mapping Theory (SMT)', properties={'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726', 'relationship_description': 'The LRME draws inspiration from SMT by incorporating its ideas into a more automated framework.'}),\n",
       "  EntityNode(label='Theory', embedding=None, properties={'id': 'Structure Mapping Theory (SMT)', 'entity_description': 'Structure Mapping Theory is a computational model that suggests analogies are based on structural correspondences between two domains. This theory has been implemented in the Structure Mapping Engine (SME', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Structure Mapping Theory (SMT)')],\n",
       " [EntityNode(label='Algorithm', embedding=None, properties={'id': 'Latent Relation Mapping Engine', 'entity_description': 'The Latent Relation Mapping Engine is an algorithm designed to automate the process of finding analogical mappings between lists of words using a large corpus of raw text. It combines ideas from Structure Mapping Theory and Latent Relational Analysis, aiming to remove the need for complex hand-coded representations.', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Latent Relation Mapping Engine'),\n",
       "  Relation(label='Integrated With', source_id='Latent Relation Mapping Engine', target_id='Latent Relational Analysis (LRA)', properties={'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726', 'relationship_description': 'LRME integrates LRA to automatically discover semantic relations among words without the need for hand-coded representations, much like how SMT is integrated with LRA in LRME.'}),\n",
       "  EntityNode(label='Analysis Technique', embedding=None, properties={'id': 'Latent Relational Analysis (LRA)', 'entity_description': 'Latent Relational Analysis is a statistical method used to uncover hidden relational structures among variables. It is combined with Structure Mapping Theory in the Latent Relation Mapping Engine (LRME', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Latent Relational Analysis (LRA)')],\n",
       " [EntityNode(label='Theory', embedding=None, properties={'id': 'Structure Mapping Theory (SMT)', 'entity_description': 'Structure Mapping Theory is a computational model that suggests analogies are based on structural correspondences between two domains. This theory has been implemented in the Structure Mapping Engine (SME', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Structure Mapping Theory (SMT)'),\n",
       "  Relation(label='Implemented In', source_id='Structure Mapping Theory (SMT)', target_id='Structure Mapping Engine (SME)', properties={'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726', 'relationship_description': 'SME is an implementation of SMT, which requires complex hand-coded representations.'}),\n",
       "  EntityNode(label='Engine', embedding=None, properties={'id': 'Structure Mapping Engine (SME)', 'entity_description': 'The Structure Mapping Engine is an implementation of the Structure Mapping Theory, which requires complex hand-coded representations to find analogical mappings.', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Structure Mapping Engine (SME)')],\n",
       " [EntityNode(label='Person', embedding=None, properties={'id': 'Garg', 'entity_description': 'A researcher or co-author involved in the work related to access control logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Garg'),\n",
       "  Relation(label='Co-Authorship', source_id='Garg', target_id='Abadi', properties={'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1', 'relationship_description': 'Both are co-authors involved in the work related to access control logics.'}),\n",
       "  EntityNode(label='Person', embedding=None, properties={'id': 'Abadi', 'entity_description': 'A researcher or co-author involved in the work related to access control logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Abadi')],\n",
       " [EntityNode(label='Person', embedding=None, properties={'id': 'Garg', 'entity_description': 'A researcher or co-author involved in the work related to access control logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Garg'),\n",
       "  Relation(label='Research Tool', source_id='Garg', target_id='LEO-II', properties={'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1', 'relationship_description': '(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.'}),\n",
       "  EntityNode(label='Software', embedding=None, properties={'id': 'LEO-II', 'entity_description': 'A higher-order theorem prover used for automated reasoning.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='LEO-II')],\n",
       " [EntityNode(label='Person', embedding=None, properties={'id': 'Abadi', 'entity_description': 'A researcher or co-author involved in the work related to access control logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Abadi'),\n",
       "  Relation(label='Research Tool', source_id='Abadi', target_id='LEO-II', properties={'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1', 'relationship_description': '(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.'}),\n",
       "  EntityNode(label='Software', embedding=None, properties={'id': 'LEO-II', 'entity_description': 'A higher-order theorem prover used for automated reasoning.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='LEO-II')],\n",
       " [EntityNode(label='Logical System', embedding=None, properties={'id': 'Modal logic S4', 'entity_description': 'A modal logic system known for its sound and complete translations of access control logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Modal logic S4'),\n",
       "  Relation(label='Subsumption', source_id='Modal logic S4', target_id='Normal Multimodal Logics', properties={'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1', 'relationship_description': '(Modal logic S4 is a part of the normal multimodal logics class, including monomodal logic K and other systems.'}),\n",
       "  EntityNode(label='Logical System', embedding=None, properties={'id': 'Normal Multimodal Logics', 'entity_description': 'A class of modal logics that includes monomodal systems K and other systems.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Normal Multimodal Logics')],\n",
       " [EntityNode(label='Logical System', embedding=None, properties={'id': 'Normal Multimodal Logics', 'entity_description': 'A class of modal logics that includes monomodal systems K and other systems.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Normal Multimodal Logics'),\n",
       "  Relation(label='Embedding', source_id='Normal Multimodal Logics', target_id='Higher-order logic', properties={'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1', 'relationship_description': '(The normal multimodal logics can be embedded within higher-order logic as demonstrated by previous work.'}),\n",
       "  EntityNode(label='Logical System', embedding=None, properties={'id': 'Higher-order logic', 'entity_description': 'A formal system used for embedding various logical systems like normal multimodal logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Higher-order logic')],\n",
       " [EntityNode(label='Networks', embedding=None, properties={'id': 'Semantic Networks', 'entity_description': 'Collections of interconnected concepts or nodes where edges qualify the relationships between these nodes. The text mentions that vertices can be \"central\" and relationships are context-dependent.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Semantic Networks'),\n",
       "  Relation(label='Contains', source_id='Semantic Networks', target_id='Vertices', properties={'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a', 'relationship_description': 'Semantic networks are described as collections of interconnected concepts or nodes, with vertices being one such element.'}),\n",
       "  EntityNode(label='Nodes', embedding=None, properties={'id': 'Vertices', 'entity_description': 'The basic elements in semantic networks, representing concepts or entities. Their centrality is subjective and depends on the relationship type.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Vertices')],\n",
       " [EntityNode(label='Metric', embedding=None, properties={'id': 'Eigenvector Centrality', 'entity_description': 'A measure of the influence of a node in a network, where its value is determined by the values of the neighboring nodes.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Eigenvector Centrality'),\n",
       "  Relation(label='Measures Influence Of', source_id='Eigenvector Centrality', target_id='Vertices', properties={'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a', 'relationship_description': 'Eigenvector centrality is defined as a measure of influence for vertices in a network.'}),\n",
       "  EntityNode(label='Nodes', embedding=None, properties={'id': 'Vertices', 'entity_description': 'The basic elements in semantic networks, representing concepts or entities. Their centrality is subjective and depends on the relationship type.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Vertices')],\n",
       " [EntityNode(label='Metric', embedding=None, properties={'id': 'PageRank', 'entity_description': 'Similar to eigenvector centrality, it ranks vertices based on their importance or influence within a network. The text notes that both metrics are used in semantic networks.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='PageRank'),\n",
       "  Relation(label='Measures Influence Of', source_id='PageRank', target_id='Vertices', properties={'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a', 'relationship_description': 'Similar to eigenvector centrality, PageRank also ranks the importance or influence of vertices within a network.'}),\n",
       "  EntityNode(label='Nodes', embedding=None, properties={'id': 'Vertices', 'entity_description': 'The basic elements in semantic networks, representing concepts or entities. Their centrality is subjective and depends on the relationship type.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Vertices')],\n",
       " [EntityNode(label='Random Walkers', embedding=None, properties={'id': 'Grammar-Based Random Walkers', 'entity_description': 'A model used in semantic networks to rank vertices based on a modified random walker constrained by a user-defined grammar.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Grammar-Based Random Walkers'),\n",
       "  Relation(label='Based On', source_id='Grammar-Based Random Walkers', target_id='Random Walkers (Markov Chain Analysis)', properties={'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a', 'relationship_description': 'The text explicitly states that the random walker model is a modified version of the random walker model used in Markov chain analysis.'}),\n",
       "  EntityNode(label='Model', embedding=None, properties={'id': 'Random Walkers (Markov Chain Analysis)', 'entity_description': 'A probabilistic model where the probability of moving from one vertex to another is defined by transition probabilities, modified here by a user-defined grammar.', 'triplet_source_id': 'a65ebbac-9058-452f-b607-33bffe930c8a'}, name='Random Walkers (Markov Chain Analysis)')],\n",
       " [EntityNode(label='Type', embedding=None, properties={'id': 'Corner Detector', 'entity_description': 'Determines how likely it is to be useful in real-world applications; involves repeatability and efficiency.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Corner Detector'),\n",
       "  Relation(label='Determines usefulness', source_id='Corner Detector', target_id='Repeatability', properties={'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7', 'relationship_description': \"The consistency of the detector's performance across different viewing positions is crucial for its real-world utility.\"}),\n",
       "  EntityNode(label='Metric', embedding=None, properties={'id': 'Repeatability', 'entity_description': 'Measure of consistency across different viewing positions of the same scene.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Repeatability')],\n",
       " [EntityNode(label='Type', embedding=None, properties={'id': 'Corner Detector', 'entity_description': 'Determines how likely it is to be useful in real-world applications; involves repeatability and efficiency.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Corner Detector'),\n",
       "  Relation(label='Determines frame rate operation', source_id='Corner Detector', target_id='Efficiency', properties={'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7', 'relationship_description': 'The efficiency affects whether the detector can operate at a high frame rate with additional processing.'}),\n",
       "  EntityNode(label='Metric', embedding=None, properties={'id': 'Efficiency', 'entity_description': 'Determine whether the detector can operate at frame rate with further processing.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Efficiency')],\n",
       " [EntityNode(label='Metric', embedding=None, properties={'id': 'Repeatability', 'entity_description': 'Measure of consistency across different viewing positions of the same scene.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Repeatability'),\n",
       "  Relation(label='Trade-off', source_id='Repeatability', target_id='Efficiency', properties={'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7', 'relationship_description': 'While repeatability ensures consistent results, it must not come at the cost of efficiency to ensure real-time operation.'}),\n",
       "  EntityNode(label='Metric', embedding=None, properties={'id': 'Efficiency', 'entity_description': 'Determine whether the detector can operate at frame rate with further processing.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Efficiency')],\n",
       " [EntityNode(label='Technique', embedding=None, properties={'id': 'Machine Learning', 'entity_description': 'Used to derive a feature detector from a new heuristic for real-time processing of live video.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Machine Learning'),\n",
       "  Relation(label='Methodology', source_id='Machine Learning', target_id='Feature Detection', properties={'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7', 'relationship_description': 'Machine learning is used as a methodology to develop the new feature detector from heuristic methods.'}),\n",
       "  EntityNode(label='Process', embedding=None, properties={'id': 'Feature Detection', 'entity_description': 'The process of identifying corners in an image or video.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Feature Detection')],\n",
       " [EntityNode(label='Process', embedding=None, properties={'id': 'Feature Detection', 'entity_description': 'The process of identifying corners in an image or video.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Feature Detection'),\n",
       "  Relation(label='Input Source', source_id='Feature Detection', target_id='Live PAL Video', properties={'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7', 'relationship_description': 'The feature detector processes live PAL video in real time.'}),\n",
       "  EntityNode(label='Input', embedding=None, properties={'id': 'Live PAL Video', 'entity_description': 'Video input processed by the feature detector for real-time application.', 'triplet_source_id': '0f9ae624-aa3a-44f5-a19a-f99d3868e7c7'}, name='Live PAL Video')],\n",
       " [EntityNode(label='Algorithm', embedding=None, properties={'id': 'Latent Relation Mapping Engine', 'entity_description': 'The Latent Relation Mapping Engine is an algorithm designed to automate the process of finding analogical mappings between lists of words using a large corpus of raw text. It combines ideas from Structure Mapping Theory and Latent Relational Analysis, aiming to remove the need for complex hand-coded representations.', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Latent Relation Mapping Engine'),\n",
       "  Relation(label='Inspired By', source_id='Latent Relation Mapping Engine', target_id='Structure Mapping Theory (SMT)', properties={'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726', 'relationship_description': 'The LRME draws inspiration from SMT by incorporating its ideas into a more automated framework.'}),\n",
       "  EntityNode(label='Theory', embedding=None, properties={'id': 'Structure Mapping Theory (SMT)', 'entity_description': 'Structure Mapping Theory is a computational model that suggests analogies are based on structural correspondences between two domains. This theory has been implemented in the Structure Mapping Engine (SME', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Structure Mapping Theory (SMT)')],\n",
       " [EntityNode(label='Theory', embedding=None, properties={'id': 'Structure Mapping Theory (SMT)', 'entity_description': 'Structure Mapping Theory is a computational model that suggests analogies are based on structural correspondences between two domains. This theory has been implemented in the Structure Mapping Engine (SME', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Structure Mapping Theory (SMT)'),\n",
       "  Relation(label='Implemented In', source_id='Structure Mapping Theory (SMT)', target_id='Structure Mapping Engine (SME)', properties={'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726', 'relationship_description': 'SME is an implementation of SMT, which requires complex hand-coded representations.'}),\n",
       "  EntityNode(label='Engine', embedding=None, properties={'id': 'Structure Mapping Engine (SME)', 'entity_description': 'The Structure Mapping Engine is an implementation of the Structure Mapping Theory, which requires complex hand-coded representations to find analogical mappings.', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Structure Mapping Engine (SME)')],\n",
       " [EntityNode(label='Algorithm', embedding=None, properties={'id': 'Latent Relation Mapping Engine', 'entity_description': 'The Latent Relation Mapping Engine is an algorithm designed to automate the process of finding analogical mappings between lists of words using a large corpus of raw text. It combines ideas from Structure Mapping Theory and Latent Relational Analysis, aiming to remove the need for complex hand-coded representations.', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Latent Relation Mapping Engine'),\n",
       "  Relation(label='Integrated With', source_id='Latent Relation Mapping Engine', target_id='Latent Relational Analysis (LRA)', properties={'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726', 'relationship_description': 'LRME integrates LRA to automatically discover semantic relations among words without the need for hand-coded representations, much like how SMT is integrated with LRA in LRME.'}),\n",
       "  EntityNode(label='Analysis Technique', embedding=None, properties={'id': 'Latent Relational Analysis (LRA)', 'entity_description': 'Latent Relational Analysis is a statistical method used to uncover hidden relational structures among variables. It is combined with Structure Mapping Theory in the Latent Relation Mapping Engine (LRME', 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}, name='Latent Relational Analysis (LRA)')],\n",
       " [EntityNode(label='Person', embedding=None, properties={'id': 'Garg', 'entity_description': 'A researcher or co-author involved in the work related to access control logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Garg'),\n",
       "  Relation(label='Co-Authorship', source_id='Garg', target_id='Abadi', properties={'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1', 'relationship_description': 'Both are co-authors involved in the work related to access control logics.'}),\n",
       "  EntityNode(label='Person', embedding=None, properties={'id': 'Abadi', 'entity_description': 'A researcher or co-author involved in the work related to access control logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Abadi')],\n",
       " [EntityNode(label='Person', embedding=None, properties={'id': 'Garg', 'entity_description': 'A researcher or co-author involved in the work related to access control logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Garg'),\n",
       "  Relation(label='Research Tool', source_id='Garg', target_id='LEO-II', properties={'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1', 'relationship_description': '(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.'}),\n",
       "  EntityNode(label='Software', embedding=None, properties={'id': 'LEO-II', 'entity_description': 'A higher-order theorem prover used for automated reasoning.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='LEO-II')],\n",
       " [EntityNode(label='Person', embedding=None, properties={'id': 'Abadi', 'entity_description': 'A researcher or co-author involved in the work related to access control logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Abadi'),\n",
       "  Relation(label='Research Tool', source_id='Abadi', target_id='LEO-II', properties={'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1', 'relationship_description': '(Garg and Abadi use LEO-II for their research on automating reasoning in access control logics.'}),\n",
       "  EntityNode(label='Software', embedding=None, properties={'id': 'LEO-II', 'entity_description': 'A higher-order theorem prover used for automated reasoning.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='LEO-II')],\n",
       " [EntityNode(label='Logical System', embedding=None, properties={'id': 'Modal logic S4', 'entity_description': 'A modal logic system known for its sound and complete translations of access control logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Modal logic S4'),\n",
       "  Relation(label='Subsumption', source_id='Modal logic S4', target_id='Normal Multimodal Logics', properties={'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1', 'relationship_description': '(Modal logic S4 is a part of the normal multimodal logics class, including monomodal logic K and other systems.'}),\n",
       "  EntityNode(label='Logical System', embedding=None, properties={'id': 'Normal Multimodal Logics', 'entity_description': 'A class of modal logics that includes monomodal systems K and other systems.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Normal Multimodal Logics')],\n",
       " [EntityNode(label='Logical System', embedding=None, properties={'id': 'Normal Multimodal Logics', 'entity_description': 'A class of modal logics that includes monomodal systems K and other systems.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Normal Multimodal Logics'),\n",
       "  Relation(label='Embedding', source_id='Normal Multimodal Logics', target_id='Higher-order logic', properties={'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1', 'relationship_description': '(The normal multimodal logics can be embedded within higher-order logic as demonstrated by previous work.'}),\n",
       "  EntityNode(label='Logical System', embedding=None, properties={'id': 'Higher-order logic', 'entity_description': 'A formal system used for embedding various logical systems like normal multimodal logics.', 'triplet_source_id': '479863de-1484-40ce-ad54-675489e194b1'}, name='Higher-order logic')]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.property_graph_store.get_triplets()\n",
    "\n",
    "# for triplet in index.property_graph_store.get_triplets():\n",
    "#     print(triplet.re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Latent Relation Mapping Engine',\n",
       " 'entity_description': 'The Latent Relation Mapping Engine is an algorithm designed to automate the process of finding analogical mappings between lists of words using a large corpus of raw text. It combines ideas from Structure Mapping Theory and Latent Relational Analysis, aiming to remove the need for complex hand-coded representations.',\n",
       " 'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.property_graph_store.get_triplets()[10][0].properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'triplet_source_id': '2d30093d-f4a3-4e8f-930f-40eaecaaa726',\n",
       " 'relationship_description': 'LRME integrates LRA to automatically discover semantic relations among words without the need for hand-coded representations, much like how SMT is integrated with LRA in LRME.'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.property_graph_store.get_triplets()[10][1].properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build communities\n",
    "\n",
    "This will create communities and summary for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.property_graph_store.build_communities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'The relationships provided describe how \"Grammar-Based Random Walkers\" and \"Random Walkers (Markov Chain Analysis)\" are interconnected, specifically through their dependency on each other. \\n\\nKey points:\\n- Both \"Grammar-Based Random Walkers\" and \"Random Walkers (Markov Chain Analysis)\" are based on a common foundational concept: the random walker model.\\n- However, \"Grammar-Based Random Walkers\" is derived as a modified version of the traditional \"Random Walker Model,\" which originates from Markov chain analysis.\\n\\nThis summary highlights that both concepts share a fundamental similarity but differ in their specific applications or modifications.',\n",
       " 1: 'In semantic networks, vertices are key components that represent interconnected concepts. These vertices can be analyzed using measures like eigenvector centrality and PageRank to evaluate their influence in the network. Both eigenvector centrality and PageRank rank the significance of vertices, with eigenvector centrality emphasizing the relative importance based on the influence of neighboring vertices, and PageRank similarly assessing vertex influence but often within the context of broader web-based networks.',\n",
       " 2: '### Summary\\n\\nThe relationships in this knowledge graph revolve around the performance metrics of a Corner Detector and their impact on its utility and operational characteristics.\\n\\n1. **Corner Detector -> Repeatability -> Determines usefulness**\\n   - The repeatability of the corner detector ensures consistent detection across different viewing positions, which is crucial for real-world application utility.\\n   \\n2. **Corner Detector -> Efficiency -> Determines frame rate operation**\\n   - Efficiency affects whether the detector can operate at high frame rates with additional processing requirements.\\n\\n3. **Repeatability -> Corner Detector (reverse)**\\n   - Ensures that consistent detection results across various viewing positions contribute to the overall usefulness of the corner detector.\\n\\n4. **Repeatability -> Efficiency -> Trade-off**\\n   - While repeatability is essential for consistent performance, it must not compromise efficiency to maintain real-time operation capabilities.\\n   \\n5. **Efficiency -> Corner Detector (reverse)**\\n   - High-efficiency levels are necessary for maintaining high frame rates and enabling the detector to handle additional processing.\\n\\n6. **Efficiency -> Repeatability -> Trade-off**\\n   - Balancing repeatability with efficiency is crucial; real-time operation requires that repeatability does not significantly reduce system efficiency.\\n\\nIn essence, these relationships highlight how both repeatability and efficiency are critical for the practical utility of a corner detector, each representing different aspects of its performance.',\n",
       " 3: 'The knowledge graph describes a relationship between `Feature Detection` and its input source, `Live PAL Video`, as well as the development methodology, which involves `Machine Learning`. \\n\\n- **Input Relationship**: The `Feature Detection` process takes real-time `Live PAL Video` as its input source. This indicates that the feature detector continuously analyzes video frames to identify features of interest.\\n  \\n- **Development Methodology**: `Machine Learning` is employed as a new methodology in developing `Feature Detection`, moving away from traditional heuristic methods. This suggests an advancement in automated and data-driven approaches for detecting specific features within videos, leveraging machine learning algorithms.\\n\\nThese relationships highlight the integration of real-time video processing with modern machine learning techniques to enhance feature detection capabilities.',\n",
       " 4: '### Summary\\n\\nThe relationships among the entities—Latent Relation Mapping Engine (LRME), Structure Mapping Theory (SMT), Latent Relational Analysis (LRA), and Structure Mapping Engine (SME)—highlight their interconnected nature in advancing automated semantic relation mapping.\\n\\n1. **Inspiration and Integration**: LRME draws inspiration from SMT by incorporating its ideas into a more automated framework, while also integrating LRA to automatically discover semantic relations without hand-coded representations.\\n2. **Implementation**: SME is an implementation of SMT, which uses complex hand-coded representations, contrasting with the automation provided by LRME in conjunction with LRA.\\n\\nThese relationships underscore how SMT and LRA have been adapted into more automated tools (LRME), while also showing that SMT itself serves as a foundational theory implemented in SME.',\n",
       " 5: 'The key relationships from the provided knowledge graph can be summarized as follows:\\n\\n- **Co-Authorship**: Garg and Abadi are co-authors involved in research related to access control logics.\\n- **Research Tool Usage**: Both Garg and Abadi utilize LEO-II, a tool for automating reasoning processes in their work on access control logics.\\n\\nThis summary highlights the collaborative nature of their academic endeavors and the specific tool they use together.',\n",
       " 6: '### Summary\\n\\nThis summary outlines the relationships between modal logic S4, normal multimodal logics, and higher-order logic.\\n\\n1. **Modal Logic S4** is a part of the broader class of **Normal Multimodal Logics**, which includes systems like monomodal logic K and other related logical frameworks.\\n2. **Higher-Order Logic** can serve as an embedding environment for **Normal Multimodal Logics**, indicating that these multimodal logics can be translated or represented within a higher-order logic framework, as supported by previous research findings.\\n\\nThese relationships highlight the hierarchical inclusion of modal logic S4 and normal multimodal logics within the broader context of logical systems, and the potential for embedding such systems into more complex frameworks like higher-order logic.'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.property_graph_store.get_community_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(\n",
    "    directed = True,\n",
    "    select_menu = True, \n",
    "    filter_menu = True, \n",
    ")\n",
    "net.show_buttons() \n",
    "net.from_nx(graph_store._create_nx_graph()) \n",
    "net.write_html('community_graph.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create QueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = GraphRAGQueryEngine(\n",
    "    graph_store=index.property_graph_store,\n",
    "    llm=llm,\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The main topics discussed across the papers are diverse, covering several key areas:\n",
       "\n",
       "1. **Vertices and Concepts**: The use of vertices as fundamental components representing interconnected concepts within semantic networks.\n",
       "2. **Network Analysis Techniques**:\n",
       "   - **Eigenvector Centrality**: Evaluates vertex significance based on their influence from neighboring vertices.\n",
       "   - **PageRank**: Assesses the relative importance or significance of vertices, particularly in web-based contexts.\n",
       "\n",
       "3. **Repeatability and Efficiency in Corner Detectors**: \n",
       "   - Repeatability ensures consistent corner detection across different conditions.\n",
       "   - Efficiency concerns computational resources and frame rate requirements for practical utility.\n",
       "\n",
       "4. **Theoretical vs. Practical Models**:\n",
       "   - Connections between theoretical models like Structure Mapping Theory (SMT) and Latent Relational Analysis (LRA).\n",
       "   - Automated semantic relation mapping using tools such as the Latent Relation Mapping Engine (LRME) and Structure Mapping Engine (SME).\n",
       "\n",
       "5. **Automated Semantic Relation Mapping**: \n",
       "   - LRME, inspired by SMT and LRA, aims to automate semantic relation discovery more efficiently.\n",
       "   - Comparison with SME’s hand-coded representations for theoretical models.\n",
       "\n",
       "These topics collectively explore the evaluation of network structure through analytical tools, practical aspects of computer vision techniques, and advancements in automated semantic analysis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What are the main topics discussed in the papers?\"\n",
    ")\n",
    "display(Markdown(f\"{response.response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided summaries, to identify papers or entities that have the most in common:\n",
       "\n",
       "1. For semantic networks and measures like eigenvector centrality (EVC) and PageRank:\n",
       "   - Common focus is on ranking vertices based on their influence using these centrality measures.\n",
       "   - Relevant papers would compare EVC and PageRank in different types of networks, particularly semantic networks, and analyze the impact of these measures in complex information systems.\n",
       "\n",
       "2. For corner detectors:\n",
       "   - The most common papers will discuss both repeatability (consistency across viewing positions) and efficiency (operational speed and resource management), providing a comprehensive understanding of detector performance.\n",
       "\n",
       "3. For Structure Mapping Theory (SMT) and Structure Mapping Engine (SME):\n",
       "   - Both share a foundational relationship where SMT is the theoretical basis for SME, an automated implementation that integrates additional capabilities but retains the core principles of SMT.\n",
       "\n",
       "In summary, the common threads are comparing measures in networks, balancing performance metrics in algorithms, and linking theoretical frameworks with practical implementations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"Which papers have the most in common?\"\n",
    ")\n",
    "display(Markdown(f\"{response.response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Markdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the main news in energy sector?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m display(\u001b[43mMarkdown\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Markdown' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "user_query = \"What are the main news in energy sector?\"\n",
    "display(Markdown(f\"{user_query}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the main news in energy sector?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m      3\u001b[0m     query\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      7\u001b[0m display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_engine' is not defined"
     ]
    }
   ],
   "source": [
    "user_query = \"What are the main news in energy sector?\"\n",
    "response = query_engine.query(\n",
    "    \"What are the main news in energy sector?\"\n",
    ")\n",
    "\n",
    "display(Markdown(f\"{user_query}\"))\n",
    "display(Markdown(f\"{response.response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "FAI_FinalProj_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
